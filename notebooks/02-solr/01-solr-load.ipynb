{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data into Solr\n",
    "\n",
    "Before doing this, we need to have Solr running. We downloaded a fresh solr-7.4.0 distribution for this, to prevent any bad interaction with existing indexes.\n",
    "\n",
    "    cd <solr_home>\n",
    "    bin/solr start\n",
    "    \n",
    "We then create a new core to hold our index.\n",
    "\n",
    "    bin/solr create -c tmdbindex \n",
    "    \n",
    "Since Solr 6.x the default similarity has changed to be BM25, and the old TF-IDF based similarity can be accessed using ClassicSimilarity. So we will declare a new field type text_tfidf that uses the TF-IDF based similarity. Any field declared (via the dynamic field naming convention) with the suffix `_t` will automatically be of type `text_general` which uses BM25 similarity. In addition, we declare a copy-field that will copy the title `title_t` and body field `description_t` (BM25 similarity) fields to their corresponding counterparts `title_tfidf` and `description_tfidf` automatically.\n",
    "\n",
    "    cd ../scripts\n",
    "    sh ./solr-schema.sh\n",
    "    \n",
    "In case you need to start over, the command to drop the core is as follows.\n",
    "\n",
    "    bin/solr delete -c tmdbindex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sqlite3\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "MOVIES_DATA = os.path.join(DATA_DIR, \"movies_metadata.csv\")\n",
    "LOOKUPS_DB = os.path.join(DATA_DIR, \"lookups.db\")\n",
    "\n",
    "SOLR_URL = \"http://localhost:8983/solr/tmdbindex/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['electricity', 'scientific experiment', 'nikola tesla']\n"
     ]
    }
   ],
   "source": [
    "def get_keywords(conn, movie_id):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select keywords from keywords where mid = ?\", [movie_id])\n",
    "    rows = cur.fetchall()\n",
    "    keywords = []\n",
    "    if len(rows) > 0:\n",
    "        for row in rows:\n",
    "            keywords = row[0].split(\"|\")\n",
    "            break\n",
    "    cur.close()\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def filter_genres(conn, genres):\n",
    "    filtered_genres = []\n",
    "    cur = conn.cursor()\n",
    "    for genre in genres:\n",
    "        cur.execute(\"select gname from genres where gname = ?\", [genre])\n",
    "        rows = cur.fetchall()\n",
    "        if len(rows) == 0:\n",
    "            continue\n",
    "        filtered_genres.append(genre)\n",
    "    cur.close()\n",
    "    return filtered_genres\n",
    "\n",
    "\n",
    "conn = sqlite3.connect(LOOKUPS_DB)\n",
    "print(get_keywords(conn, 460870))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records ingested into Solr\n",
      "1000 records ingested into Solr\n",
      "2000 records ingested into Solr\n",
      "3000 records ingested into Solr\n",
      "4000 records ingested into Solr\n",
      "5000 records ingested into Solr\n",
      "6000 records ingested into Solr\n",
      "7000 records ingested into Solr\n",
      "8000 records ingested into Solr\n",
      "9000 records ingested into Solr\n",
      "10000 records ingested into Solr\n",
      "11000 records ingested into Solr\n",
      "12000 records ingested into Solr\n",
      "13000 records ingested into Solr\n",
      "14000 records ingested into Solr\n",
      "15000 records ingested into Solr\n",
      "16000 records ingested into Solr\n",
      "17000 records ingested into Solr\n",
      "18000 records ingested into Solr\n",
      "19000 records ingested into Solr\n",
      "20000 records ingested into Solr\n",
      "21000 records ingested into Solr\n",
      "22000 records ingested into Solr\n",
      "23000 records ingested into Solr\n",
      "24000 records ingested into Solr\n",
      "25000 records ingested into Solr\n",
      "26000 records ingested into Solr\n",
      "27000 records ingested into Solr\n",
      "28000 records ingested into Solr\n",
      "29000 records ingested into Solr\n",
      "30000 records ingested into Solr\n",
      "31000 records ingested into Solr\n",
      "32000 records ingested into Solr\n",
      "33000 records ingested into Solr\n",
      "34000 records ingested into Solr\n",
      "35000 records ingested into Solr\n",
      "36000 records ingested into Solr\n",
      "37000 records ingested into Solr\n",
      "38000 records ingested into Solr\n",
      "39000 records ingested into Solr\n",
      "40000 records ingested into Solr\n",
      "41000 records ingested into Solr\n",
      "42000 records ingested into Solr\n",
      "43000 records ingested into Solr\n",
      "44000 records ingested into Solr\n",
      "45000 records ingested into Solr\n",
      "45466 records ingested into Solr, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "def get_float(orig_value, default_value):\n",
    "    if orig_value is None:\n",
    "        return default_value\n",
    "    elif len(orig_value.strip()) == 0:\n",
    "        return default_value\n",
    "    else:\n",
    "        return float(orig_value)\n",
    "    \n",
    "def parse_genres(genre_json):\n",
    "    if len(genre_json.strip()) == 0:\n",
    "        return []\n",
    "    names = []\n",
    "    idname_pairs = json.loads(genre_json.replace(\"'\", \"\\\"\"))\n",
    "    for idname_pair in idname_pairs:\n",
    "        names.append(idname_pair[\"name\"])\n",
    "    return names\n",
    "\n",
    "\n",
    "def add_record_to_solr(solr_url, doc_id, title, description, popularity, \n",
    "                       release_date, revenue, runtime, rating, keywords, genres,\n",
    "                       should_commit=False):\n",
    "    headers = {\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"accept\": \"application/json\"\n",
    "    }\n",
    "    if doc_id is None:\n",
    "        # only do a commit\n",
    "        requests.post(solr_url + \"update\", params={\"commit\": \"true\"}, headers=headers)\n",
    "    else:\n",
    "        req_body = json.dumps({\n",
    "            \"add\": {\n",
    "                \"doc\": {\n",
    "                    \"id\": doc_id,\n",
    "                    \"title_t\": title,\n",
    "                    \"description_t\": description,\n",
    "                    \"popularity_f\": popularity,\n",
    "                    \"released_dt\": release_date,\n",
    "                    \"revenue_f\": revenue,\n",
    "                    \"runtime_f\": runtime,\n",
    "                    \"rating_f\": rating,\n",
    "                    \"keywords_ss\": keywords,\n",
    "                    \"genres_ss\": genres\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        params = { \"commit\": \"true\" if should_commit else \"false\" }\n",
    "        requests.post(solr_url + \"update\", data=req_body, params=params, headers=headers)\n",
    "        \n",
    "\n",
    "i = 0\n",
    "should_commit = False\n",
    "with open(MOVIES_DATA, \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"{:d} records ingested into Solr\".format(i))\n",
    "            should_commit = True\n",
    "        if row[\"original_language\"] != \"en\":\n",
    "            # only stick to english\n",
    "            i += 1\n",
    "            continue\n",
    "        doc_id = int(row[\"id\"])\n",
    "        title = row[\"original_title\"]\n",
    "        description = row[\"overview\"]\n",
    "        popularity = get_float(row[\"popularity\"], 0.0)\n",
    "        release_date = row[\"release_date\"]\n",
    "        revenue = get_float(row[\"revenue\"], 0.0)\n",
    "        runtime = get_float(row[\"runtime\"], 0.0)\n",
    "        rating = get_float(row[\"vote_average\"], 0.0)\n",
    "        # look up keywords\n",
    "        keywords = get_keywords(conn, doc_id)\n",
    "        # parse out genres\n",
    "        genres = filter_genres(conn, parse_genres(row[\"genres\"]))\n",
    "        # add record to solr\n",
    "        add_record_to_solr(SOLR_URL, doc_id, title, description, popularity, \n",
    "                           release_date, revenue, runtime, rating, keywords, genres,\n",
    "                           should_commit=should_commit)\n",
    "        should_commit = False\n",
    "        i += 1\n",
    "\n",
    "add_record_to_solr(SOLR_URL, None, None, None, None, None, None, None, None, None, None, True)\n",
    "print(\"{:d} records ingested into Solr, COMPLETE\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
