{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (Solr LTR)\n",
    "\n",
    "We train a LambdaMart model using [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib%20How%20to%20use/) using its command line interface as shown below:\n",
    "\n",
    "Command is as follows:\n",
    "\n",
    "    cd <scripts_dir>\n",
    "    java -jar RankLib-2.1-patched.jar \\\n",
    "        -train ../data/solr_features_train.txt \\\n",
    "        -test ../data/solr_features_test.txt \\\n",
    "        -validate ../data/solr_features_val.txt \\\n",
    "        -ranker 6 \\\n",
    "        -metric2t NDCG@10 \\\n",
    "        -metric2T ERR@10 \\\n",
    "        -save ../data/solr_lambdamart_model.txt\n",
    "\n",
    "Console output is shown below:\n",
    "\n",
    "\t[+] General Parameters:\n",
    "\tTraining data:\t../data/solr_features_train.txt\n",
    "\tTest data:\t../data/solr_features_test.txt\n",
    "\tValidation data:\t../data/solr_features_val.txt\n",
    "\tFeature vector representation: Dense.\n",
    "\tRanking method:\tLambdaMART\n",
    "\tFeature description file:\tUnspecified. All features will be used.\n",
    "\tTrain metric:\tNDCG@10\n",
    "\tTest metric:\tERR@10\n",
    "\tHighest relevance label (to compute ERR): 4\n",
    "\tFeature normalization: No\n",
    "\tModel file: ../data/solr_lambdamart_model.txt\n",
    "\t\n",
    "\t[+] LambdaMART's Parameters:\n",
    "\tNo. of trees: 1000\n",
    "\tNo. of leaves: 10\n",
    "\tNo. of threshold candidates: 256\n",
    "\tLearning rate: 0.1\n",
    "\tStop early: 100 rounds without performance gain on validation data\n",
    "\t\n",
    "\tReading feature file [../data/solr_features_train.txt]... [Done.]            \n",
    "\t(12 ranked lists, 1088 entries read)\n",
    "\tReading feature file [../data/solr_features_val.txt]... [Done.]            \n",
    "\t(3 ranked lists, 210 entries read)\n",
    "\tReading feature file [../data/solr_features_test.txt]... [Done.]            \n",
    "\t(5 ranked lists, 404 entries read)\n",
    "\tInitializing... [Done]\n",
    "\t---------------------------------\n",
    "\tTraining starts...\n",
    "\t---------------------------------\n",
    "\t#iter   | NDCG@10-T | NDCG@10-V | \n",
    "\t---------------------------------\n",
    "\t1       | 0.5041    | 0.5524    | \n",
    "\t2       | 0.6176    | 0.565     | \n",
    "\t3       | 0.6316    | 0.5732    | \n",
    "\t4       | 0.6296    | 0.5976    | \n",
    "\t5       | 0.64      | 0.6229    | \n",
    "\t6       | 0.6382    | 0.614     | \n",
    "\t7       | 0.6462    | 0.6491    | \n",
    "\t8       | 0.6465    | 0.6491    | \n",
    "\t9       | 0.652     | 0.6491    | \n",
    "\t10      | 0.6478    | 0.6491    | \n",
    "\t11      | 0.6538    | 0.6491    | \n",
    "\t12      | 0.6481    | 0.6491    | \n",
    "\t13      | 0.6546    | 0.6491    | \n",
    "\t14      | 0.659     | 0.6422    | \n",
    "\t15      | 0.6573    | 0.6416    | \n",
    "\t16      | 0.6655    | 0.6416    | \n",
    "\t17      | 0.6556    | 0.6584    | \n",
    "\t18      | 0.6563    | 0.6584    | \n",
    "\t19      | 0.65      | 0.6582    | \n",
    "\t20      | 0.6747    | 0.6644    | \n",
    "\t21      | 0.6753    | 0.6864    | \n",
    "\t22      | 0.6771    | 0.6955    | \n",
    "\t23      | 0.6825    | 0.6955    | \n",
    "\t24      | 0.7044    | 0.6945    | \n",
    "\t25      | 0.7107    | 0.6796    | \n",
    "\t26      | 0.7124    | 0.6796    | \n",
    "\t27      | 0.7108    | 0.6796    | \n",
    "\t28      | 0.7128    | 0.6816    | \n",
    "\t29      | 0.7257    | 0.6974    | \n",
    "\t30      | 0.7248    | 0.698     | \n",
    "\t31      | 0.733     | 0.698     | \n",
    "\t32      | 0.7321    | 0.6982    | \n",
    "\t33      | 0.7397    | 0.6991    | \n",
    "\t34      | 0.7459    | 0.6991    | \n",
    "\t35      | 0.7546    | 0.7017    | \n",
    "\t36      | 0.7679    | 0.7017    | \n",
    "\t37      | 0.7674    | 0.7017    | \n",
    "\t38      | 0.7641    | 0.7017    | \n",
    "\t39      | 0.7659    | 0.7099    | \n",
    "\t40      | 0.7738    | 0.7099    | \n",
    "\t41      | 0.7737    | 0.7091    | \n",
    "\t42      | 0.7748    | 0.7091    | \n",
    "\t43      | 0.7798    | 0.7091    | \n",
    "\t44      | 0.7788    | 0.7211    | \n",
    "\t45      | 0.7817    | 0.7085    | \n",
    "\t46      | 0.7787    | 0.7091    | \n",
    "\t47      | 0.7794    | 0.7091    | \n",
    "\t48      | 0.7834    | 0.7091    | \n",
    "\t49      | 0.7823    | 0.7033    | \n",
    "\t50      | 0.7919    | 0.7027    | \n",
    "\t51      | 0.7974    | 0.7027    | \n",
    "\t52      | 0.8079    | 0.6943    | \n",
    "\t53      | 0.8095    | 0.6931    | \n",
    "\t54      | 0.8137    | 0.6943    | \n",
    "\t55      | 0.8217    | 0.6931    | \n",
    "\t56      | 0.8194    | 0.6785    | \n",
    "\t57      | 0.8199    | 0.6791    | \n",
    "\t58      | 0.8207    | 0.6791    | \n",
    "\t59      | 0.8172    | 0.6806    | \n",
    "\t60      | 0.8197    | 0.6952    | \n",
    "\t61      | 0.8213    | 0.6986    | \n",
    "\t62      | 0.8366    | 0.7007    | \n",
    "\t63      | 0.8359    | 0.7071    | \n",
    "\t64      | 0.8399    | 0.705     | \n",
    "\t65      | 0.8382    | 0.705     | \n",
    "\t66      | 0.8366    | 0.7369    | \n",
    "\t67      | 0.8427    | 0.7381    | \n",
    "\t68      | 0.8457    | 0.7053    | \n",
    "\t69      | 0.8381    | 0.7395    | \n",
    "\t70      | 0.8485    | 0.7067    | \n",
    "\t71      | 0.8503    | 0.7087    | \n",
    "\t72      | 0.8468    | 0.7153    | \n",
    "\t73      | 0.8512    | 0.7154    | \n",
    "\t74      | 0.8504    | 0.7083    | \n",
    "\t75      | 0.8583    | 0.709     | \n",
    "\t76      | 0.8633    | 0.6999    | \n",
    "\t77      | 0.8606    | 0.7069    | \n",
    "\t78      | 0.862     | 0.6999    | \n",
    "\t79      | 0.8636    | 0.6984    | \n",
    "\t80      | 0.8645    | 0.6984    | \n",
    "\t81      | 0.864     | 0.6981    | \n",
    "\t82      | 0.8606    | 0.6986    | \n",
    "\t83      | 0.8621    | 0.6986    | \n",
    "\t84      | 0.8673    | 0.698     | \n",
    "\t85      | 0.8666    | 0.6991    | \n",
    "\t86      | 0.8641    | 0.6981    | \n",
    "\t87      | 0.8636    | 0.6981    | \n",
    "\t88      | 0.8594    | 0.7011    | \n",
    "\t89      | 0.8717    | 0.7006    | \n",
    "\t90      | 0.8798    | 0.7067    | \n",
    "\t91      | 0.8761    | 0.705     | \n",
    "\t92      | 0.8755    | 0.7109    | \n",
    "\t93      | 0.8752    | 0.7047    | \n",
    "\t94      | 0.8736    | 0.7053    | \n",
    "\t95      | 0.8741    | 0.7373    | \n",
    "\t96      | 0.8791    | 0.7381    | \n",
    "\t97      | 0.8788    | 0.7365    | \n",
    "\t98      | 0.8842    | 0.7369    | \n",
    "\t99      | 0.8851    | 0.7362    | \n",
    "\t100     | 0.8845    | 0.7367    | \n",
    "\t101     | 0.8942    | 0.7355    | \n",
    "\t102     | 0.8934    | 0.7064    | \n",
    "\t103     | 0.8988    | 0.7065    | \n",
    "\t104     | 0.8972    | 0.7072    | \n",
    "\t105     | 0.8966    | 0.7072    | \n",
    "\t106     | 0.8954    | 0.7072    | \n",
    "\t107     | 0.8965    | 0.7072    | \n",
    "\t108     | 0.9001    | 0.7064    | \n",
    "\t109     | 0.8947    | 0.7064    | \n",
    "\t110     | 0.8953    | 0.7057    | \n",
    "\t111     | 0.8953    | 0.7057    | \n",
    "\t112     | 0.8989    | 0.7196    | \n",
    "\t113     | 0.9016    | 0.7196    | \n",
    "\t114     | 0.903     | 0.7196    | \n",
    "\t115     | 0.9021    | 0.7181    | \n",
    "\t116     | 0.9034    | 0.7181    | \n",
    "\t117     | 0.9029    | 0.7181    | \n",
    "\t118     | 0.9046    | 0.7181    | \n",
    "\t119     | 0.9037    | 0.7181    | \n",
    "\t120     | 0.9102    | 0.7181    | \n",
    "\t121     | 0.9038    | 0.7181    | \n",
    "\t122     | 0.9091    | 0.7179    | \n",
    "\t123     | 0.9072    | 0.7179    | \n",
    "\t124     | 0.9105    | 0.7179    | \n",
    "\t125     | 0.9105    | 0.7182    | \n",
    "\t126     | 0.9155    | 0.7185    | \n",
    "\t127     | 0.9113    | 0.7185    | \n",
    "\t128     | 0.9104    | 0.7185    | \n",
    "\t129     | 0.9133    | 0.7185    | \n",
    "\t130     | 0.9087    | 0.7153    | \n",
    "\t131     | 0.9165    | 0.7148    | \n",
    "\t132     | 0.9145    | 0.7148    | \n",
    "\t133     | 0.9209    | 0.7142    | \n",
    "\t134     | 0.9147    | 0.7142    | \n",
    "\t135     | 0.9211    | 0.7142    | \n",
    "\t136     | 0.9154    | 0.7148    | \n",
    "\t137     | 0.9171    | 0.7148    | \n",
    "\t138     | 0.9159    | 0.7148    | \n",
    "\t139     | 0.9178    | 0.7031    | \n",
    "\t140     | 0.9196    | 0.7037    | \n",
    "\t141     | 0.9155    | 0.7037    | \n",
    "\t142     | 0.9184    | 0.7023    | \n",
    "\t143     | 0.927     | 0.7023    | \n",
    "\t144     | 0.9317    | 0.7054    | \n",
    "\t145     | 0.9383    | 0.707     | \n",
    "\t146     | 0.9384    | 0.707     | \n",
    "\t147     | 0.9386    | 0.7075    | \n",
    "\t148     | 0.9418    | 0.7075    | \n",
    "\t149     | 0.9402    | 0.7077    | \n",
    "\t150     | 0.9393    | 0.7037    | \n",
    "\t151     | 0.9402    | 0.7037    | \n",
    "\t152     | 0.9378    | 0.7045    | \n",
    "\t153     | 0.9403    | 0.7175    | \n",
    "\t154     | 0.94      | 0.7207    | \n",
    "\t155     | 0.9435    | 0.7175    | \n",
    "\t156     | 0.9461    | 0.7175    | \n",
    "\t157     | 0.9479    | 0.7175    | \n",
    "\t158     | 0.9462    | 0.7207    | \n",
    "\t159     | 0.9462    | 0.7143    | \n",
    "\t160     | 0.9483    | 0.7143    | \n",
    "\t161     | 0.9481    | 0.7182    | \n",
    "\t162     | 0.9529    | 0.7217    | \n",
    "\t163     | 0.95      | 0.7217    | \n",
    "\t164     | 0.9501    | 0.7184    | \n",
    "\t165     | 0.9515    | 0.722     | \n",
    "\t166     | 0.9522    | 0.7221    | \n",
    "\t167     | 0.9515    | 0.7221    | \n",
    "\t168     | 0.9517    | 0.7221    | \n",
    "\t169     | 0.955     | 0.7223    | \n",
    "\t170     | 0.955     | 0.7223    | \n",
    "\t---------------------------------\n",
    "\tFinished sucessfully.\n",
    "\tNDCG@10 on training data: 0.8381\n",
    "\tNDCG@10 on validation data: 0.7395\n",
    "\t---------------------------------\n",
    "\tERR@10 on test data: 0.8157\n",
    "\t\n",
    "\tModel saved to: ../data/tmdb-dataset/solr_lambdamart_model.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert RankLib XML Model to JSON\n",
    "\n",
    "Tried using [Doug Turnbull's script for his ES Demo](https://github.com/o19s/elasticsearch-learning-to-rank/blob/7426858c2afb168ac426cab6d857fddccb9c26fc/demo/ranklibToJson.py) but it didn't work for me, so I built one to convert the XML to the format described in the Javadocs for the Solr LTR [MultipleAdditiveTreesModel](https://lucene.apache.org/solr/7_4_0//solr-ltr/org/apache/solr/ltr/model/MultipleAdditiveTreesModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "RANKLIB_LM_MODEL = os.path.join(DATA_DIR, \"solr_lambdamart_model.txt\")\n",
    "RANKLIB_PROC_LM_MODEL = os.path.join(DATA_DIR, \"solr_lambdamart_model.xml\")\n",
    "SOLR_LM_MODEL = os.path.join(DATA_DIR, \"solr_lambdamart_model.json\")\n",
    "\n",
    "FEATURE_LIST = [\n",
    "    \"origScore\", \"titleSimTFIDF\", \"titleSimBM25\", \"descSimTFIDF\", \"descSimBM25\",\n",
    "    \"docRecency\", \"isGoHands\", \"isAniplex\", \"isThriller\", \"isForeign\",\n",
    "    \"isDrama\", \"isWar\", \"isAction\", \"isComedy\", \"isMusic\", \n",
    "    \"isRomance\", \"isAdventure\", \"isFamily\", \"isFantasy\", \"isCrime\",\n",
    "    \"isHorror\", \"isHistory\", \"isMystery\", \"isAnimation\", \"isDocumentary\",\n",
    "    \"isWestern\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraw = open(RANKLIB_LM_MODEL, \"r\")\n",
    "fxml = open(RANKLIB_PROC_LM_MODEL, \"w\")\n",
    "fxml.write(\"<?xml version=\\\"1.0\\\"?>\\n\")\n",
    "for line in fraw:\n",
    "    if line.startswith(\"##\") or len(line.strip()) == 0:\n",
    "        continue\n",
    "    fxml.write(\"{:s}\".format(line))\n",
    "fxml.close()\n",
    "fraw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_split(el_split, feature_id2name, split_type=\"root\"):\n",
    "    if split_type != \"root\":\n",
    "        split_type = el_split.attrib[\"pos\"]\n",
    "    output = el_split.find(\"output\")\n",
    "    if output is not None:\n",
    "        return {\n",
    "            \"value\": output.text.strip()\n",
    "        }\n",
    "    feature = feature_id2name[int(el_split.find(\"feature\").text.strip())]\n",
    "    threshold = el_split.find(\"threshold\").text.strip()\n",
    "    el_csplits = el_split.findall(\"split\")\n",
    "    for el_csplit in el_csplits:\n",
    "        attr_pos = el_csplit.attrib[\"pos\"]\n",
    "        if attr_pos == \"left\":\n",
    "            left = parse_split(el_csplit, feature_id2name, \"left\")\n",
    "        elif attr_pos == \"right\":\n",
    "            right = parse_split(el_csplit, feature_id2name, \"right\")\n",
    "    return {\n",
    "        \"feature\": feature,\n",
    "        \"threshold\": threshold,\n",
    "        \"left\": left,\n",
    "        \"right\": right\n",
    "    }\n",
    "\n",
    "\n",
    "trees = []\n",
    "feature_id2name = {i+1:f for i, f in enumerate(FEATURE_LIST)}\n",
    "xml = ET.parse(RANKLIB_PROC_LM_MODEL)\n",
    "el_ensemble = xml.getroot()\n",
    "for el_tree in el_ensemble:\n",
    "    weight = el_tree.attrib[\"weight\"]\n",
    "    el_split = el_tree.find(\"split\")\n",
    "    tree_dict = {\n",
    "        \"weight\": weight,\n",
    "        \"root\": parse_split(el_split, feature_id2name)\n",
    "    }\n",
    "    trees.append(tree_dict)\n",
    "params_dict = {\"trees\" : trees}\n",
    "    \n",
    "features = [{\"name\": f} for f in FEATURE_LIST]\n",
    "model_dict = {\n",
    "    \"store\": \"myFeatureStore\",\n",
    "    \"name\": \"myLambdaMARTModel\",\n",
    "    \"class\": \"org.apache.solr.ltr.model.MultipleAdditiveTreesModel\",\n",
    "    \"features\": features,\n",
    "    \"params\": params_dict\n",
    "}\n",
    "with open(SOLR_LM_MODEL, \"w\") as fjson:\n",
    "    fjson.write(json.dumps(model_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model to Solr\n",
    "\n",
    "We use the following curl command from command line to upload the JSON output file `solr_lambdamart_model.json`. Note that we have kept the values for threshold and weight as strings instead of floats (using floats gives a spurious error saying that MultipleAdditiveTreesModel is not available).\n",
    "\n",
    "    curl -XPUT \"http://localhost:8983/solr/tmdbindex/schema/model-store\" \\\n",
    "        --data-binary \"@solr_lambdamart_model.json\" -H \"Content-type:application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
