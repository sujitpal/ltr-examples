{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTR Case Study: Elasticsearch\n",
    "\n",
    "We need Elasticsearch 6.3 installed, it can be [downloaded from here](https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.2.tar.gz). To start Elasticsearch, run the command:\n",
    "\n",
    "    cd <elasticsearch_home>\n",
    "    bin/elasticsearch\n",
    "    \n",
    "The case study follows the steps outlined in the [Elasticsearch LTR Documentation](https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/core-concepts.html) and the tutorials from [Pere Urbon-Bayes](https://medium.com/@purbon/learning-to-rank-101-5755f2797a3a) and [Doug Turnbull](https://github.com/o19s/elasticsearch-learning-to-rank/tree/master/demo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "MODEL_DIR = \"../../models\"\n",
    "\n",
    "MOVIES_DATA = os.path.join(DATA_DIR, \"movies_metadata.csv\")\n",
    "LOOKUPS_DB = os.path.join(DATA_DIR, \"lookups.db\")\n",
    "FEATURE_FILE_TEMPLATE = os.path.join(DATA_DIR, \"es_features_{:s}.txt\")\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"es_lambdamart_model.txt\")\n",
    "\n",
    "FEATURE_LIST = [\n",
    "    \"origScore\", \"titleSimTFIDF\", \"titleSimBM25\", \"descSimTFIDF\", \"descSimBM25\",\n",
    "    \"docRecency\", \"isGoHands\", \"isAniplex\", \"isThriller\", \"isForeign\",\n",
    "    \"isDrama\", \"isWar\", \"isAction\", \"isComedy\", \"isMusic\", \n",
    "    \"isRomance\", \"isAdventure\", \"isFamily\", \"isFantasy\", \"isCrime\",\n",
    "    \"isHorror\", \"isHistory\", \"isMystery\", \"isAnimation\", \"isDocumentary\",\n",
    "    \"isWestern\"\n",
    "]\n",
    "QUERY_LIST = [\n",
    "    \"murder\", \"musical\", \"biography\", \"police\", \"world war ii\",\n",
    "    \"comedy\", \"superhero\", \"nazis\", \"romance\", \"martial arts\",\n",
    "    \"extramarital\", \"spy\", \"vampire\", \"magic\", \"wedding\",\n",
    "    \"sport\", \"prison\", \"teacher\", \"alien\", \"dystopia\"\n",
    "]\n",
    "\n",
    "ES_URL = \"http://localhost:9200\"\n",
    "TOP_N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Plugin\n",
    "\n",
    "We are using ES-LTR plugin for ES 6.3. LTR plugin is installed using the following command:\n",
    "\n",
    "    cd <elasticsearch_home>\n",
    "    bin/elasticsearch-plugin install http://es-learn-to-rank.labs.o19s.com/ltr-1.1.0-es6.3.1.zip\n",
    "    \n",
    "We are using [elasticsearch-head](https://github.com/mobz/elasticsearch-head) as our browser client. In order to make it work, you need to disable some security measures using the following directives in `config/elasticsearch.yml`.\n",
    "\n",
    "    http.cors.enabled: true\n",
    "    http.cors.allow-origin: \"*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### Create Index and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tmdbindex\"}\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"settings\" : {\n",
    "        \"index\" : {\n",
    "            \"number_of_shards\" : 1, \n",
    "            \"number_of_replicas\" : 1 \n",
    "        }\n",
    "    },\n",
    "    \"mappings\" : {\n",
    "        \"doc\" : {\n",
    "            \"properties\" : {\n",
    "                \"doc_id\": { \"type\": \"keyword\", \"store\": \"true\" },\n",
    "                \"title\": { \"type\": \"text\", \"store\": \"true\", \"copy_to\": \"title_tfidf\" },\n",
    "                \"description\": { \"type\": \"text\", \"store\": \"true\", \"copy_to\": \"description_tfidf\" },\n",
    "                \"title_tfidf\": { \"type\": \"text\", \"store\": \"true\", \"similarity\": \"classic\" },\n",
    "                \"description_tfidf\": { \"type\": \"text\", \"store\": \"true\", \"similarity\": \"classic\" },\n",
    "                \"popularity\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"release_dt\": { \"type\": \"date\", \"store\": \"true\" },\n",
    "                \"revenue\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"runtime\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"rating\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"keywords\": { \"type\": \"keyword\", \"store\": \"true\" },\n",
    "                \"genres\": { \"type\": \"keyword\", \"store\": \"true\" }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp = requests.put(ES_URL + \"/tmdbindex\", headers=headers, data=json.dumps(data))\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(conn, movie_id):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select keywords from keywords where mid = ?\", [movie_id])\n",
    "    rows = cur.fetchall()\n",
    "    keywords = []\n",
    "    if len(rows) > 0:\n",
    "        for row in rows:\n",
    "            keywords = row[0].split(\"|\")\n",
    "            break\n",
    "    cur.close()\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def filter_genres(conn, genres):\n",
    "    filtered_genres = []\n",
    "    cur = conn.cursor()\n",
    "    for genre in genres:\n",
    "        cur.execute(\"select gname from genres where gname = ?\", [genre])\n",
    "        rows = cur.fetchall()\n",
    "        if len(rows) == 0:\n",
    "            continue\n",
    "        filtered_genres.append(genre)\n",
    "    cur.close()\n",
    "    return filtered_genres\n",
    "\n",
    "\n",
    "def get_float(orig_value, default_value):\n",
    "    if orig_value is None:\n",
    "        return default_value\n",
    "    elif len(orig_value.strip()) == 0:\n",
    "        return default_value\n",
    "    else:\n",
    "        return float(orig_value)\n",
    "\n",
    "\n",
    "def parse_genres(genre_json):\n",
    "    if len(genre_json.strip()) == 0:\n",
    "        return []\n",
    "    names = []\n",
    "    idname_pairs = json.loads(genre_json.replace(\"'\", \"\\\"\"))\n",
    "    for idname_pair in idname_pairs:\n",
    "        names.append(idname_pair[\"name\"])\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_record_to_es(es_url, doc_id, title, description, popularity, \n",
    "                     release_date, revenue, runtime, rating, keywords, genres,\n",
    "                     should_commit=False):\n",
    "    if doc_id is not None:\n",
    "        doc = {\n",
    "            \"doc_id\": str(doc_id),\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"popularity\": popularity,\n",
    "            \"release_dt\": release_date,\n",
    "            \"revenue\": revenue,\n",
    "            \"runtime\": runtime,\n",
    "            \"rating\": rating,\n",
    "            \"keywords\": keywords,\n",
    "            \"genres\": genres\n",
    "        }\n",
    "        resp = requests.put(es_url + \"/tmdbindex/doc/{:d}\".format(doc_id), \n",
    "                            headers=headers, data=json.dumps(doc))\n",
    "    if should_commit:\n",
    "        requests.post(es_url + \"/tmdbindex/_flush\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records ingested into Elasticsearch\n",
      "1000 records ingested into Elasticsearch\n",
      "2000 records ingested into Elasticsearch\n",
      "3000 records ingested into Elasticsearch\n",
      "4000 records ingested into Elasticsearch\n",
      "5000 records ingested into Elasticsearch\n",
      "6000 records ingested into Elasticsearch\n",
      "7000 records ingested into Elasticsearch\n",
      "8000 records ingested into Elasticsearch\n",
      "9000 records ingested into Elasticsearch\n",
      "10000 records ingested into Elasticsearch\n",
      "11000 records ingested into Elasticsearch\n",
      "12000 records ingested into Elasticsearch\n",
      "13000 records ingested into Elasticsearch\n",
      "14000 records ingested into Elasticsearch\n",
      "15000 records ingested into Elasticsearch\n",
      "16000 records ingested into Elasticsearch\n",
      "17000 records ingested into Elasticsearch\n",
      "18000 records ingested into Elasticsearch\n",
      "19000 records ingested into Elasticsearch\n",
      "20000 records ingested into Elasticsearch\n",
      "21000 records ingested into Elasticsearch\n",
      "22000 records ingested into Elasticsearch\n",
      "23000 records ingested into Elasticsearch\n",
      "24000 records ingested into Elasticsearch\n",
      "25000 records ingested into Elasticsearch\n",
      "26000 records ingested into Elasticsearch\n",
      "27000 records ingested into Elasticsearch\n",
      "28000 records ingested into Elasticsearch\n",
      "29000 records ingested into Elasticsearch\n",
      "30000 records ingested into Elasticsearch\n",
      "31000 records ingested into Elasticsearch\n",
      "32000 records ingested into Elasticsearch\n",
      "33000 records ingested into Elasticsearch\n",
      "34000 records ingested into Elasticsearch\n",
      "35000 records ingested into Elasticsearch\n",
      "36000 records ingested into Elasticsearch\n",
      "37000 records ingested into Elasticsearch\n",
      "38000 records ingested into Elasticsearch\n",
      "39000 records ingested into Elasticsearch\n",
      "40000 records ingested into Elasticsearch\n",
      "41000 records ingested into Elasticsearch\n",
      "42000 records ingested into Elasticsearch\n",
      "43000 records ingested into Elasticsearch\n",
      "44000 records ingested into Elasticsearch\n",
      "45000 records ingested into Elasticsearch\n",
      "45466 records ingested into Elasticsearch, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(LOOKUPS_DB)\n",
    "i = 0\n",
    "should_commit = False\n",
    "with open(MOVIES_DATA, \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"{:d} records ingested into Elasticsearch\".format(i))\n",
    "            should_commit = True\n",
    "        if row[\"original_language\"] != \"en\":\n",
    "            # only stick to english\n",
    "            i += 1\n",
    "            continue\n",
    "        doc_id = int(row[\"id\"])\n",
    "        title = row[\"original_title\"]\n",
    "        description = row[\"overview\"]\n",
    "        popularity = get_float(row[\"popularity\"], 0.0)\n",
    "        release_date = row[\"release_date\"]\n",
    "        revenue = get_float(row[\"revenue\"], 0.0)\n",
    "        runtime = get_float(row[\"runtime\"], 0.0)\n",
    "        rating = get_float(row[\"vote_average\"], 0.0)\n",
    "        # look up keywords\n",
    "        keywords = get_keywords(conn, doc_id)\n",
    "        # parse out genres\n",
    "        genres = filter_genres(conn, parse_genres(row[\"genres\"]))\n",
    "        # add record to solr\n",
    "        add_record_to_es(ES_URL, doc_id, title, description, popularity, \n",
    "                         release_date, revenue, runtime, rating, keywords, genres,\n",
    "                         should_commit=should_commit)\n",
    "        should_commit = False\n",
    "        i += 1\n",
    "\n",
    "add_record_to_es(ES_URL, None, None, None, None, None, None, None, None, None, None, True)\n",
    "print(\"{:d} records ingested into Elasticsearch, COMPLETE\".format(i))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LTR features\n",
    "\n",
    "### Initialize Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\".ltrstore\"}\n"
     ]
    }
   ],
   "source": [
    "requests.delete(ES_URL + \"/_ltr\")\n",
    "resp = requests.put(ES_URL + \"/_ltr\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature definition\n",
    "\n",
    "We then construct our features and POST them into a named feature store. A validation query is used to make sure that our queries are well-formed. See [the ES LTR docs](https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/building-features.html) for more information. One thing to note is that all the templates are really queries (the stuff that goes under the \"query\" key in the normal search JSON requests), so you can build most of them by referring to the ES online query docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\".ltrstore\",\"_type\":\"store\",\"_id\":\"featureset-myFeatures\",\"_version\":1,\"result\":\"created\",\"forced_refresh\":true,\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"_seq_no\":0,\"_primary_term\":1}\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"validation\": {\n",
    "        \"params\": {\n",
    "            \"query\": \"martial arts\"\n",
    "        },\n",
    "        \"index\": \"tmdbindex\"\n",
    "    },\n",
    "    \"featureset\": {\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"name\": \"origScore\",\n",
    "                \"params\": [\n",
    "                    \"query\"\n",
    "                ],\n",
    "                \"template_language\": \"mustache\",\n",
    "                \"template\": {\n",
    "                    \"dis_max\": {\n",
    "                        \"queries\": [\n",
    "                            { \n",
    "                                \"match\": { \n",
    "                                    \"title\": \"{{query}}\"\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"match\": {\n",
    "                                    \"description\":  \"{{query}}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"titleSimTFIDF\",\n",
    "                \"params\": [\n",
    "                    \"query\"\n",
    "                ],\n",
    "                \"template_language\": \"mustache\",\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                        \"title_tfidf\": \"{{query}}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"titleSimBM25\",\n",
    "                \"params\": [\n",
    "                    \"query\"\n",
    "                ],\n",
    "                \"template_language\": \"mustache\",\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                        \"title\": \"{{query}}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"descSimTFIDF\",\n",
    "                \"params\": [\n",
    "                    \"query\"\n",
    "                ],\n",
    "                \"template_language\": \"mustache\",\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                        \"description_tfidf\": \"{{query}}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"descSimBM25\",\n",
    "                \"params\": [\n",
    "                    \"query\"\n",
    "                ],\n",
    "                \"template_language\": \"mustache\",\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                        \"description\": \"{{query}}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"docRecency\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"function_score\": {\n",
    "                        \"field_value_factor\": {\n",
    "                            \"field\": \"release_dt\",\n",
    "                            \"factor\": 3.16e-11,\n",
    "                            \"modifier\": \"reciprocal\",\n",
    "                            \"missing\": 1\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isGoHands\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"GoHands\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isAniplex\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Aniplex\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isThriller\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Thriller\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isForeign\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Foreign\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isDrama\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Drama\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isWar\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"War\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isAction\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Action\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isComedy\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Comedy\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isMusic\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Music\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isRomance\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Romance\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isAdventure\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Adventure\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isFamily\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Family\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isFantasy\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Fantasy\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isCrime\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Crime\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isHorror\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Horror\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isHistory\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"History\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isMystery\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Mystery\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isAnimation\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Animation\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isDocumentary\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Documentary\" \n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"isWestern\",\n",
    "                \"params\": [],\n",
    "                \"template\": {\n",
    "                    \"term\" : { \n",
    "                        \"genres\" : \"Western\" \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "resp = requests.post(ES_URL + \"/_ltr/_featureset/myFeatures\", headers=headers, \n",
    "                     data=json.dumps(data))\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureset-myFeatures\n"
     ]
    }
   ],
   "source": [
    "# list all featuresets\n",
    "resp = requests.get(ES_URL + \"/_ltr/_featureset\", headers=headers)\n",
    "resp_json = json.loads(resp.text)\n",
    "# print(resp_json)\n",
    "for doc in resp_json[\"hits\"][\"hits\"]:\n",
    "    print(doc[\"_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LTR Features\n",
    "\n",
    "We will generate resultsets for 20 queries, and split them up into training, validation and test sets. Labels are generated by applying a transformation on the rating (continuous values 0-10) to transform them into a 5-level categorical label. Judgement lists of query, features and labels are written out in LETOR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_docids_for_query(query):\n",
    "    data = {\n",
    "        \"query\": {\n",
    "            \"dis_max\": {\n",
    "                \"queries\": [\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"title\": \"%s\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"match\": {\n",
    "                            \"description\": \"%s\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"from\": 0,\n",
    "        \"size\": 100\n",
    "    }\n",
    "    resp = requests.post(ES_URL + \"/tmdbindex/_search\", headers=headers, data=json.dumps(data))\n",
    "    resp_json = json.loads(resp.text)\n",
    "    doc_ids = []\n",
    "    for doc in resp_json[\"hits\"][\"hits\"]:\n",
    "        doc_id = doc[\"_source\"][\"doc_id\"]\n",
    "        doc_ids.append(doc_id)\n",
    "    return doc_ids\n",
    "\n",
    "\n",
    "doc_ids = collect_docids_for_query(\"martial arts\")\n",
    "assert(len(doc_ids) <= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating2label(rating):\n",
    "    \"\"\" convert 0-10 continuous rating to 1-5 categorical labels \"\"\"\n",
    "    return int(rating // 2) + 1\n",
    "\n",
    "assert(rating2label(6.4) == 4)\n",
    "assert(rating2label(9.8) == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name2id = {name: idx + 1 for idx, name in enumerate(FEATURE_LIST)}\n",
    "\n",
    "assert(feature_name2id[\"isRomance\"] == 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features_for_docids(query, doc_ids, feature_name2id):\n",
    "    data = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"filter\": [\n",
    "                    {\n",
    "                        \"terms\": {\n",
    "                            \"_id\": doc_ids\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"sltr\": {\n",
    "                            \"_name\": \"logged_featureset\",\n",
    "                            \"featureset\": \"myFeatures\",\n",
    "                            \"params\": {\n",
    "                                \"query\": query\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"ext\": {\n",
    "            \"ltr_log\": {\n",
    "                \"log_specs\": {\n",
    "                    \"name\": \"main\",\n",
    "                    \"named_query\": \"logged_featureset\",\n",
    "                    \"missing_as_zero\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"from\": 0,\n",
    "        \"size\": 100\n",
    "    }\n",
    "    resp = requests.post(ES_URL + \"/tmdbindex/_search\", headers=headers, data=json.dumps(data))\n",
    "    resp_json = json.loads(resp.text)\n",
    "    features = {}\n",
    "    for doc in resp_json[\"hits\"][\"hits\"]:\n",
    "        doc_src = doc[\"_source\"]\n",
    "        doc_id = doc_src[\"doc_id\"]\n",
    "        rating = doc_src[\"rating\"]\n",
    "        label = rating2label(rating)\n",
    "        letor_feats = []\n",
    "        doc_feats = doc[\"fields\"][\"_ltrlog\"][0][\"main\"]\n",
    "        for feat_nv in doc_feats:\n",
    "            feat_name = feat_nv[\"name\"]\n",
    "            feat_id = feature_name2id[feat_name]\n",
    "            feat_value = feat_nv[\"value\"]\n",
    "            letor_feats.append(\"{:d}:{:.3f}\".format(feat_id, feat_value))\n",
    "        features[doc_id] = (label, \" \".join(letor_feats))\n",
    "    return features\n",
    "\n",
    "\n",
    "feats = collect_features_for_docids(\"martial arts\", ['35405', '34068', '13492'], feature_name2id)\n",
    "assert(len(feats) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_letor(fout, doc_ids, features, qid, query):\n",
    "    for doc_id in doc_ids:\n",
    "        label, feat_str = features[doc_id]\n",
    "        fout.write(\"{:d} qid:{:d} {:s} # docid:{:s} query:{:s}\\n\".format(\n",
    "            label, qid, feat_str, doc_id, query))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating feature for vampire (train)\n",
      "generating feature for prison (train)\n",
      "generating feature for musical (train)\n",
      "generating feature for extramarital (train)\n",
      "generating feature for spy (train)\n",
      "generating feature for world war ii (train)\n",
      "generating feature for biography (train)\n",
      "generating feature for martial arts (train)\n",
      "generating feature for murder (train)\n",
      "generating feature for police (train)\n",
      "generating feature for sport (train)\n",
      "generating feature for alien (train)\n",
      "generating feature for dystopia (val)\n",
      "generating feature for magic (val)\n",
      "generating feature for comedy (val)\n",
      "generating feature for nazis (test)\n",
      "generating feature for superhero (test)\n",
      "generating feature for wedding (test)\n",
      "generating feature for romance (test)\n",
      "generating feature for teacher (test)\n",
      "number of queries, train 12, test 5, validation 3\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(QUERY_LIST)\n",
    "train_queries = QUERY_LIST[0:12]\n",
    "val_queries = QUERY_LIST[12:15]\n",
    "test_queries = QUERY_LIST[15:]\n",
    "feat_suffixes = [\"train\", \"val\", \"test\"]\n",
    "qid = 1\n",
    "for qt_idx, queries in enumerate([train_queries, val_queries, test_queries]):\n",
    "    fletor = open(FEATURE_FILE_TEMPLATE.format(feat_suffixes[qt_idx]), \"w\")\n",
    "    for query in queries:\n",
    "        print(\"generating feature for {:s} ({:s})\".format(query, feat_suffixes[qt_idx]))\n",
    "        # collect doc_ids for query\n",
    "        doc_ids = collect_docids_for_query(query)\n",
    "        # return features for each doc_id\n",
    "        features = collect_features_for_docids(query, doc_ids, feature_name2id)\n",
    "        print_letor(fletor, doc_ids, features, qid, query)\n",
    "        qid += 1\n",
    "        \n",
    "print(\"number of queries, train {:d}, test {:d}, validation {:d}\".format(\n",
    "    len(train_queries), len(test_queries), len(val_queries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LTR Model using RankLib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to train RankLib on command line with LETOR files generated in previous step.\n",
    "\n",
    "    java -jar RankLib-2.10.jar \\\n",
    "        -train ../data/es_features_train.txt \\\n",
    "        -test ../data/es_features_test.txt \\\n",
    "        -validate ../data/es_features_val.txt \\\n",
    "        -ranker 6 \\\n",
    "        -metric2t NDCG@10 \\\n",
    "        -metric2T NDCG@10 \\\n",
    "        -save ../models/es_lambdamart_model.txt\n",
    "\n",
    "Command output is as follows:\n",
    "\n",
    "    Discard orig. features\n",
    "    Training data:\t../data/es_features_train.txt\n",
    "    Test data:\t../data/es_features_test.txt\n",
    "    Validation data:\t../data/es_features_val.txt\n",
    "    Feature vector representation: Dense.\n",
    "    Ranking method:\tLambdaMART\n",
    "    Feature description file:\tUnspecified. All features will be used.\n",
    "    Train metric:\tNDCG@10\n",
    "    Test metric:\tNDCG@10\n",
    "    Feature normalization: No\n",
    "    Model file: ../models/es_lambdamart_model.txt\n",
    "    \n",
    "    [+] LambdaMART's Parameters:\n",
    "    No. of trees: 1000\n",
    "    No. of leaves: 10\n",
    "    No. of threshold candidates: 256\n",
    "    Min leaf support: 1\n",
    "    Learning rate: 0.1\n",
    "    Stop early: 100 rounds without performance gain on validation data\n",
    "    \n",
    "    Reading feature file [../data/es_features_train.txt]... [Done.]            \n",
    "    (12 ranked lists, 1200 entries read)\n",
    "    Reading feature file [../data/es_features_val.txt]... [Done.]            \n",
    "    (3 ranked lists, 300 entries read)\n",
    "    Reading feature file [../data/es_features_test.txt]... [Done.]            \n",
    "    (5 ranked lists, 480 entries read)\n",
    "    Initializing... [Done]\n",
    "    ---------------------------------\n",
    "    Training starts...\n",
    "    ---------------------------------\n",
    "    #iter   | NDCG@10-T | NDCG@10-V | \n",
    "    ---------------------------------\n",
    "    1       | 0.844     | 0.844     | \n",
    "    2       | 0.8652    | 0.8652    | \n",
    "    3       | 0.8652    | 0.8652    | \n",
    "    4       | 0.8652    | 0.8652    | \n",
    "    5       | 0.8652    | 0.8652    | \n",
    "    6       | 0.8652    | 0.8652    | \n",
    "    7       | 0.8652    | 0.8652    | \n",
    "    8       | 0.8652    | 0.8652    | \n",
    "    9       | 0.8652    | 0.8652    | \n",
    "    10      | 0.8652    | 0.8652    | \n",
    "    11      | 0.8652    | 0.8652    | \n",
    "    12      | 0.8652    | 0.8652    | \n",
    "    13      | 0.8997    | 0.8997    | \n",
    "    14      | 0.8997    | 0.8997    | \n",
    "    15      | 0.9011    | 0.9011    | \n",
    "    16      | 0.9011    | 0.9011    | \n",
    "    17      | 0.9028    | 0.9028    | \n",
    "    18      | 0.9028    | 0.9028    | \n",
    "    19      | 0.9373    | 0.9373    | \n",
    "    20      | 0.9373    | 0.9373    | \n",
    "    21      | 0.9373    | 0.9373    | \n",
    "    22      | 0.9435    | 0.9435    | \n",
    "    23      | 0.9607    | 0.9607    | \n",
    "    24      | 0.9607    | 0.9607    | \n",
    "    25      | 0.978     | 0.978     | \n",
    "    26      | 0.9801    | 0.9801    | \n",
    "    27      | 0.9865    | 0.9865    | \n",
    "    28      | 0.9917    | 0.9917    | \n",
    "    29      | 0.9917    | 0.9917    | \n",
    "    30      | 0.9917    | 0.9917    | \n",
    "    31      | 0.9917    | 0.9917    | \n",
    "    32      | 0.9917    | 0.9917    | \n",
    "    33      | 1.0       | 1.0       | \n",
    "    34      | 1.0       | 1.0       | \n",
    "    35      | 1.0       | 1.0       | \n",
    "    36      | 1.0       | 1.0       | \n",
    "    37      | 1.0       | 1.0       | \n",
    "    38      | 1.0       | 1.0       | \n",
    "    39      | 1.0       | 1.0       | \n",
    "    40      | 1.0       | 1.0       | \n",
    "    41      | 1.0       | 1.0       | \n",
    "    42      | 1.0       | 1.0       | \n",
    "    43      | 1.0       | 1.0       | \n",
    "    44      | 1.0       | 1.0       | \n",
    "    45      | 1.0       | 1.0       | \n",
    "    46      | 1.0       | 1.0       | \n",
    "    47      | 1.0       | 1.0       | \n",
    "    48      | 1.0       | 1.0       | \n",
    "    49      | 1.0       | 1.0       | \n",
    "    50      | 1.0       | 1.0       | \n",
    "    51      | 1.0       | 1.0       | \n",
    "    52      | 1.0       | 1.0       | \n",
    "    53      | 1.0       | 1.0       | \n",
    "    54      | 1.0       | 1.0       | \n",
    "    55      | 1.0       | 1.0       | \n",
    "    56      | 1.0       | 1.0       | \n",
    "    57      | 1.0       | 1.0       | \n",
    "    58      | 1.0       | 1.0       | \n",
    "    59      | 1.0       | 1.0       | \n",
    "    60      | 1.0       | 1.0       | \n",
    "    61      | 1.0       | 1.0       | \n",
    "    62      | 1.0       | 1.0       | \n",
    "    63      | 1.0       | 1.0       | \n",
    "    64      | 1.0       | 1.0       | \n",
    "    65      | 1.0       | 1.0       | \n",
    "    66      | 1.0       | 1.0       | \n",
    "    67      | 1.0       | 1.0       | \n",
    "    68      | 1.0       | 1.0       | \n",
    "    69      | 1.0       | 1.0       | \n",
    "    70      | 1.0       | 1.0       | \n",
    "    71      | 1.0       | 1.0       | \n",
    "    72      | 1.0       | 1.0       | \n",
    "    73      | 1.0       | 1.0       | \n",
    "    74      | 1.0       | 1.0       | \n",
    "    75      | 1.0       | 1.0       | \n",
    "    76      | 1.0       | 1.0       | \n",
    "    77      | 1.0       | 1.0       | \n",
    "    78      | 1.0       | 1.0       | \n",
    "    79      | 1.0       | 1.0       | \n",
    "    80      | 1.0       | 1.0       | \n",
    "    81      | 1.0       | 1.0       | \n",
    "    82      | 1.0       | 1.0       | \n",
    "    83      | 1.0       | 1.0       | \n",
    "    84      | 1.0       | 1.0       | \n",
    "    85      | 1.0       | 1.0       | \n",
    "    86      | 1.0       | 1.0       | \n",
    "    87      | 1.0       | 1.0       | \n",
    "    88      | 1.0       | 1.0       | \n",
    "    89      | 1.0       | 1.0       | \n",
    "    90      | 1.0       | 1.0       | \n",
    "    91      | 1.0       | 1.0       | \n",
    "    92      | 1.0       | 1.0       | \n",
    "    93      | 1.0       | 1.0       | \n",
    "    94      | 1.0       | 1.0       | \n",
    "    95      | 1.0       | 1.0       | \n",
    "    96      | 1.0       | 1.0       | \n",
    "    97      | 1.0       | 1.0       | \n",
    "    98      | 1.0       | 1.0       | \n",
    "    99      | 1.0       | 1.0       | \n",
    "    100     | 1.0       | 1.0       | \n",
    "    101     | 1.0       | 1.0       | \n",
    "    102     | 1.0       | 1.0       | \n",
    "    103     | 1.0       | 1.0       | \n",
    "    104     | 1.0       | 1.0       | \n",
    "    105     | 1.0       | 1.0       | \n",
    "    106     | 1.0       | 1.0       | \n",
    "    107     | 1.0       | 1.0       | \n",
    "    108     | 1.0       | 1.0       | \n",
    "    109     | 1.0       | 1.0       | \n",
    "    110     | 1.0       | 1.0       | \n",
    "    111     | 1.0       | 1.0       | \n",
    "    112     | 1.0       | 1.0       | \n",
    "    113     | 1.0       | 1.0       | \n",
    "    114     | 1.0       | 1.0       | \n",
    "    115     | 1.0       | 1.0       | \n",
    "    116     | 1.0       | 1.0       | \n",
    "    117     | 1.0       | 1.0       | \n",
    "    118     | 1.0       | 1.0       | \n",
    "    119     | 1.0       | 1.0       | \n",
    "    120     | 1.0       | 1.0       | \n",
    "    121     | 1.0       | 1.0       | \n",
    "    122     | 1.0       | 1.0       | \n",
    "    123     | 1.0       | 1.0       | \n",
    "    124     | 1.0       | 1.0       | \n",
    "    125     | 1.0       | 1.0       | \n",
    "    126     | 1.0       | 1.0       | \n",
    "    127     | 1.0       | 1.0       | \n",
    "    128     | 1.0       | 1.0       | \n",
    "    129     | 1.0       | 1.0       | \n",
    "    130     | 1.0       | 1.0       | \n",
    "    131     | 1.0       | 1.0       | \n",
    "    132     | 1.0       | 1.0       | \n",
    "    133     | 1.0       | 1.0       | \n",
    "    134     | 1.0       | 1.0       | \n",
    "    ---------------------------------\n",
    "    Finished sucessfully.\n",
    "    NDCG@10 on training data: 1.0\n",
    "    NDCG@10 on validation data: 1.0\n",
    "    ---------------------------------\n",
    "    NDCG@10 on test data: 0.9962\n",
    "    \n",
    "    Model saved to: ../models/es_lambdamart_model.txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_index\":\".ltrstore\",\"_type\":\"store\",\"_id\":\"model-es_lambdamart_model\",\"_version\":1,\"result\":\"created\",\"forced_refresh\":true,\"_shards\":{\"total\":1,\"successful\":1,\"failed\":0},\"_seq_no\":1,\"_primary_term\":1}\n"
     ]
    }
   ],
   "source": [
    "model_def = None\n",
    "with open(MODEL_FILE, \"r\") as model_file:\n",
    "    model_def = model_file.read()\n",
    "\n",
    "data = {\n",
    "    \"model\": {\n",
    "        \"name\": \"es_lambdamart_model\",\n",
    "        \"model\": {\n",
    "            \"type\": \"model/ranklib\",\n",
    "            \"definition\": model_def\n",
    "        }\n",
    "    }\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "resp = requests.post(ES_URL + \"/_ltr/_featureset/myFeatures/_createmodel\", \n",
    "                     headers=headers, data=json.dumps(data))\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run rerank Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★★☆☆\n",
      "★★★★☆\n"
     ]
    }
   ],
   "source": [
    "def rating2label(rating):\n",
    "    \"\"\" convert 0-10 continuous rating to 1-5 categorical labels \"\"\"\n",
    "    if rating == 10.0:\n",
    "        rating -= 0.01\n",
    "    return int(rating // 2) + 1\n",
    "\n",
    "\n",
    "def get_rating_string(rating):\n",
    "    rating_string = []\n",
    "    for i in range(rating):\n",
    "        rating_string.append(u\"\\u2605\")\n",
    "    for i in range(5 - rating):\n",
    "        rating_string.append(u\"\\u2606\")\n",
    "    return \"\".join(rating_string)\n",
    "\n",
    "\n",
    "print(get_rating_string(3))\n",
    "print(get_rating_string(rating2label(6.4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = QUERY_LIST[random.randint(0, len(QUERY_LIST))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_results(docs, query, top_n):\n",
    "    print(\"top {:d} results for {:s}\".format(TOP_N * 2, query))\n",
    "    print(\"---\")\n",
    "    for doc in docs:\n",
    "        doc_id, title, rating, score = doc\n",
    "        stars = get_rating_string(rating2label(rating))\n",
    "        print(\"{:s} {:06d} {:.3f} {:s}\".format(stars, int(doc_id), score, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 results without re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 results for murder\n",
      "---\n",
      "★★★★★ 407992 8.166 MURDER and murder\n",
      "★★★★☆ 031930 8.162 Murder!\n",
      "★★★★☆ 176841 6.878 Moonlight Murder\n",
      "★★★☆☆ 031043 6.878 Mike's Murder\n",
      "★★★☆☆ 013561 6.878 Murder Party\n",
      "★★★★☆ 000758 6.878 Murder Ahoy\n",
      "★★★★☆ 015375 6.878 Murder, Inc.\n",
      "★★☆☆☆ 089345 6.878 Bloody Murder\n",
      "★★★★☆ 360626 6.878 Prescription: Murder\n",
      "★★★★☆ 047401 6.878 Sky Murder\n",
      "★★★☆☆ 009415 5.942 Murder at 1600\n",
      "★★★★☆ 010440 5.942 Manhattan Murder Mystery\n",
      "★★★★☆ 001965 5.942 A Perfect Murder\n",
      "★★★★☆ 001834 5.942 Murder, My Sweet\n",
      "★★★★☆ 006037 5.942 Murder by Death\n",
      "★★★★☆ 011892 5.942 Murder by Numbers\n",
      "★★★★☆ 034374 5.942 Murder by Decree\n",
      "★★★★☆ 000750 5.942 Murder She Said\n",
      "★★★★☆ 000757 5.942 Murder Most Foul\n",
      "★★★★☆ 018930 5.942 Murder by Contract\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"query\": {\n",
    "        \"dis_max\": {\n",
    "            \"queries\": [\n",
    "                { \"match\": { \"title\": query }},\n",
    "                { \"match\": { \"body\":  query }}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"from\": 0,\n",
    "    \"size\": TOP_N * 2\n",
    "}\n",
    "resp = requests.post(ES_URL + \"/tmdbindex/_search\", headers=headers, data=json.dumps(data))\n",
    "resp_json = json.loads(resp.text)\n",
    "result_docs = []\n",
    "for doc in resp_json[\"hits\"][\"hits\"]:\n",
    "    doc_src = doc[\"_source\"]\n",
    "    doc_id = doc_src[\"doc_id\"]\n",
    "    rating = doc_src[\"rating\"]\n",
    "    title = doc_src[\"title\"]\n",
    "    score = doc[\"_score\"]\n",
    "    result_docs.append((doc_id, title, rating, score))\n",
    "render_results(result_docs, query, TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 results with LTR reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 results for murder\n",
      "---\n",
      "★★★★★ 407992 4.722 MURDER and murder\n",
      "★★★★☆ 031930 4.664 Murder!\n",
      "★★★★☆ 053947 4.637 The Murder of Fred Hampton\n",
      "★★★★☆ 368835 4.551 Murder Rap: Inside the Biggie and Tupac Murders\n",
      "★★☆☆☆ 089345 3.898 Bloody Murder\n",
      "★★★☆☆ 031043 3.789 Mike's Murder\n",
      "★★★☆☆ 013561 3.606 Murder Party\n",
      "★★★★☆ 015375 3.435 Murder, Inc.\n",
      "★★★★☆ 006037 3.414 Murder by Death\n",
      "★★★★☆ 000758 3.392 Murder Ahoy\n",
      "★★★★☆ 360626 3.312 Prescription: Murder\n",
      "★★★☆☆ 009415 3.232 Murder at 1600\n",
      "★★★★☆ 176841 3.208 Moonlight Murder\n",
      "★★★★☆ 047401 3.208 Sky Murder\n",
      "★★★★☆ 010440 2.973 Manhattan Murder Mystery\n",
      "★★★★☆ 001965 2.757 A Perfect Murder\n",
      "★★★★☆ 011892 2.757 Murder by Numbers\n",
      "★★★☆☆ 108282 2.755 Murder on Flight 502\n",
      "★★★★☆ 038962 2.684 Murder by Proxy\n",
      "★★★★☆ 034374 2.636 Murder by Decree\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"query\": {\n",
    "        \"dis_max\": {\n",
    "            \"queries\": [\n",
    "                { \"match\": { \"title\": query }},\n",
    "                { \"match\": { \"body\":  query }}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"rescore\": {\n",
    "        \"window_size\": 100,\n",
    "        \"query\": {\n",
    "            \"rescore_query\": {\n",
    "                \"sltr\": {\n",
    "                    \"params\": {\n",
    "                        \"query\": query\n",
    "                    },\n",
    "                    \"model\": \"es_lambdamart_model\",\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"from\": 0,\n",
    "    \"size\": TOP_N * 2\n",
    "}\n",
    "resp = requests.post(ES_URL + \"/tmdbindex/_search\", headers=headers, data=json.dumps(data))\n",
    "resp_json = json.loads(resp.text)\n",
    "result_docs = []\n",
    "for doc in resp_json[\"hits\"][\"hits\"]:\n",
    "    doc_src = doc[\"_source\"]\n",
    "    doc_id = doc_src[\"doc_id\"]\n",
    "    rating = doc_src[\"rating\"]\n",
    "    title = doc_src[\"title\"]\n",
    "    score = doc[\"_score\"]\n",
    "    result_docs.append((doc_id, title, rating, score))\n",
    "render_results(result_docs, query, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
