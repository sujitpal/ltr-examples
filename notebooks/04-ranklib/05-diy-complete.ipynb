{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTR Case Study: DIY\n",
    "\n",
    "Here DIY is taken to mean an index that does not offer any native/plugin based support for LTR. Specifically, the index has no built-in support for generating feature values (although this can certainly be faked for some features).\n",
    "\n",
    "Advantages are that you are no longer constrained by the types of features your plugin can give you. Disadvantages are that you have to work harder to generate features.\n",
    "\n",
    "In our case, we will use the same Solr index we ran our Solr+LTR case study against, except that we will not depend on the index for any LTR support.\n",
    "\n",
    "Start the Solr server with the following command:\n",
    "\n",
    "    bin/solr start -Dsolr.ltr.enabled=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import datetime\n",
    "import gensim\n",
    "import json\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import spacy\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data\"\n",
    "MODEL_DIR = \"../../models\"\n",
    "\n",
    "SOLR_URL = \"http://localhost:8983/solr/tmdbindex\"\n",
    "FEATURE_FILE_TEMPLATE = os.path.join(DATA_DIR, \"diy_features_{:s}.txt\")\n",
    "SCORE_FILE = os.path.join(DATA_DIR, \"diy_lambdamart_scores.txt\")\n",
    "\n",
    "FEATURE_LIST = [\n",
    "    \"origScore\", \"titleSimTFIDF\", \"titleSimBM25\", \"descSimTFIDF\", \"descSimBM25\",\n",
    "    \"docRecency\", \"isGoHands\", \"isAniplex\", \"isThriller\", \"isForeign\",\n",
    "    \"isDrama\", \"isWar\", \"isAction\", \"isComedy\", \"isMusic\", \n",
    "    \"isRomance\", \"isAdventure\", \"isFamily\", \"isFantasy\", \"isCrime\",\n",
    "    \"isHorror\", \"isHistory\", \"isMystery\", \"isAnimation\", \"isDocumentary\",\n",
    "    \"isWestern\"\n",
    "]\n",
    "QUERY_LIST = [\n",
    "    \"murder\", \"musical\", \"biography\", \"police\", \"world war ii\",\n",
    "    \"comedy\", \"superhero\", \"nazis\", \"romance\", \"martial arts\",\n",
    "    \"extramarital\", \"spy\", \"vampire\", \"magic\", \"wedding\",\n",
    "    \"sport\", \"prison\", \"teacher\", \"alien\", \"dystopia\"\n",
    "]\n",
    "TOP_N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Plugin\n",
    "\n",
    "Not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We already have data in this index, so we will just reuse that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LTR features\n",
    "\n",
    "Since we are not using index support, we do not need to define our features to the index, so we can skip this step as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LTR features\n",
    "\n",
    "We will use the same features as our Solr case study, but we will generate the feature values using our own code outside the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_results(query, num_docs):\n",
    "    payload = {\n",
    "        \"q\": query,\n",
    "        \"defType\": \"edismax\",\n",
    "        \"qf\": \"title_t description_t\",\n",
    "        \"pf\": \"title_t description_t\",\n",
    "        \"mm\": 2,\n",
    "        \"fl\": \"*,score\",            \n",
    "        \"rows\": num_docs\n",
    "    }\n",
    "    params = urllib.parse.urlencode(payload, quote_via=urllib.parse.quote_plus)\n",
    "    search_url = SOLR_URL + \"/select?\" + params\n",
    "    resp = requests.get(search_url)\n",
    "    resp_json = json.loads(resp.text)\n",
    "    docs = resp_json[\"response\"][\"docs\"]\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = get_search_results(\"martial arts\", 100)\n",
    "assert(len(docs) <= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_similarities(query, docs, field_name):\n",
    "    fields = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            fields.append(doc[field_name])\n",
    "        except KeyError:\n",
    "            fields.append(\" \")\n",
    "    tfidf = TfidfVectorizer()\n",
    "    field_vecs = tfidf.fit_transform(fields)\n",
    "    query_vec = np.sum(tfidf.transform(query.split(\" \")), axis=0)\n",
    "    sims = linear_kernel(query_vec, field_vecs).flatten()\n",
    "    tfidf = None\n",
    "    return sims\n",
    "\n",
    "\n",
    "desc_sims_tfidf = get_tfidf_similarities(\"martial arts\", docs, \"description_t\")\n",
    "assert(len(desc_sims_tfidf) == len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25_similarities(query, docs, field_name):\n",
    "    \"\"\" Code adapted from:\n",
    "        https://stackoverflow.com/questions/40966014/how-to-use-gensim-bm25-ranking-in-python\n",
    "    \"\"\"\n",
    "    fields = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            fields.append(nlp(doc[field_name].lower()))\n",
    "        except KeyError:\n",
    "            fields.append(nlp(\" \"))\n",
    "    field_tokens = []\n",
    "    for field in fields:\n",
    "        field_tokens.append([token.text for token in field])\n",
    "    dictionary = gensim.corpora.Dictionary(field_tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in field_tokens]\n",
    "    bm25 = gensim.summarization.bm25.BM25(corpus)\n",
    "    avg_idf = sum(float(val) for val in bm25.idf.values()) / len(bm25.idf)\n",
    "    query_tokens = [token.text for token in nlp(query.lower())]\n",
    "    query_vec = dictionary.doc2bow(query_tokens)\n",
    "    sims = bm25.get_scores(query_vec, avg_idf)\n",
    "    dictionary, corpus, bm25 = None, None, None\n",
    "    return sims\n",
    "\n",
    "\n",
    "desc_sims_bm25 = get_bm25_similarities(\"martial arts\", docs, \"description_t\")\n",
    "assert(len(desc_sims_bm25) == len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Recency\n",
    "\n",
    "We will get this as the number of seconds since epoch divided by 365\\*24\\*60\\*60 (so decimal value in years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_recencies(docs, field_name):\n",
    "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "    recencies = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            field = doc[field_name]\n",
    "            field_dttm = datetime.datetime.strptime(doc[field_name], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            total_years = (field_dttm - epoch).total_seconds() / (365 * 24 * 60 * 60)\n",
    "        except KeyError:\n",
    "            total_years = 0\n",
    "        recencies.append(total_years)\n",
    "    return recencies\n",
    "\n",
    "doc_recencies = get_doc_recencies(docs, \"released_dt\")\n",
    "assert(len(doc_recencies) == len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_categories(docs):\n",
    "    categories = []\n",
    "    for doc in docs:\n",
    "        category_dict = {}\n",
    "        try:\n",
    "            genres = set(doc[\"genres_ss\"])\n",
    "            for feature in FEATURE_LIST[6:]:\n",
    "                feat_val = feature[2:]\n",
    "                if feat_val in genres:\n",
    "                    category_dict[feature] = 1\n",
    "                else:\n",
    "                    category_dict[feature] = 0\n",
    "        except KeyError:\n",
    "            category_dict = {feature: 0 for feature in FEATURE_LIST[6:]}\n",
    "        categories.append(category_dict)\n",
    "    return categories\n",
    "\n",
    "categories = get_doc_categories(docs)\n",
    "assert(len(categories) == len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating2label(rating):\n",
    "    \"\"\" convert 0-10 continuous rating to 1-5 categorical labels \"\"\"\n",
    "    return int(rating // 2) + 1\n",
    "\n",
    "assert(rating2label(6.4) == 4)\n",
    "assert(rating2label(9.8) == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name2id = {name: idx + 1 for idx, name in enumerate(FEATURE_LIST)}\n",
    "\n",
    "assert(feature_name2id[\"isRomance\"] == 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_letor(doc_id, rating, qid, query, orig_score, title_sim_tfidf, desc_sim_tfidf,\n",
    "                 title_sim_bm25, desc_sim_bm25, doc_recency, doc_categories):\n",
    "    label = rating2label(rating)\n",
    "    features = {\n",
    "        \"origScore\": \"{:.5f}\".format(orig_score),\n",
    "        \"titleSimTFIDF\": \"{:.5f}\".format(title_sim_tfidf),\n",
    "        \"titleSimBM25\": \"{:.5f}\".format(title_sim_bm25),\n",
    "        \"descSimTFIDF\": \"{:.5f}\".format(desc_sim_tfidf),\n",
    "        \"descSimBM25\": \"{:.5f}\".format(desc_sim_bm25),\n",
    "        \"docRecency\": \"{:.5f}\".format(doc_recency),\n",
    "        \"isGoHands\": \"{:.3f}\".format(doc_categories[\"isGoHands\"]),\n",
    "        \"isAniplex\": \"{:.3f}\".format(doc_categories[\"isAniplex\"]),\n",
    "        \"isThriller\": \"{:.3f}\".format(doc_categories[\"isThriller\"]),\n",
    "        \"isForeign\": \"{:.3f}\".format(doc_categories[\"isForeign\"]),\n",
    "        \"isDrama\": \"{:.3f}\".format(doc_categories[\"isDrama\"]),\n",
    "        \"isWar\": \"{:.3f}\".format(doc_categories[\"isWar\"]),  \n",
    "        \"isAction\": \"{:.3f}\".format(doc_categories[\"isAction\"]),\n",
    "        \"isComedy\": \"{:.3f}\".format(doc_categories[\"isComedy\"]),\n",
    "        \"isMusic\": \"{:.3f}\".format(doc_categories[\"isMusic\"]),\n",
    "        \"isRomance\": \"{:.3f}\".format(doc_categories[\"isRomance\"]),\n",
    "        \"isAdventure\": \"{:.3f}\".format(doc_categories[\"isAdventure\"]),\n",
    "        \"isFamily\": \"{:.3f}\".format(doc_categories[\"isFamily\"]),\n",
    "        \"isFantasy\": \"{:.3f}\".format(doc_categories[\"isFantasy\"]),\n",
    "        \"isCrime\": \"{:.3f}\".format(doc_categories[\"isCrime\"]),\n",
    "        \"isHorror\": \"{:.3f}\".format(doc_categories[\"isHorror\"]),\n",
    "        \"isHistory\": \"{:.3f}\".format(doc_categories[\"isHistory\"]),\n",
    "        \"isMystery\": \"{:.3f}\".format(doc_categories[\"isMystery\"]),\n",
    "        \"isAnimation\": \"{:.3f}\".format(doc_categories[\"isAnimation\"]),\n",
    "        \"isDocumentary\": \"{:.3f}\".format(doc_categories[\"isDocumentary\"]),\n",
    "        \"isWestern\": \"{:.3f}\".format(doc_categories[\"isWestern\"])\n",
    "    }\n",
    "    feat_pairs = []\n",
    "    for feat_name in FEATURE_LIST:\n",
    "        feat_id = str(feature_name2id[feat_name])\n",
    "        feat_val = features[feat_name]\n",
    "        feat_pairs.append(\":\".join([feat_id, feat_val]))\n",
    "    return \"{:d} qid:{:d} {:s} # docid:{:d} query:{:s}\".format(\n",
    "        label, qid, \" \".join(feat_pairs), doc_id, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating feature for sport (train)\n",
      "generating feature for nazis (train)\n",
      "generating feature for teacher (train)\n",
      "generating feature for world war ii (train)\n",
      "generating feature for spy (train)\n",
      "generating feature for vampire (train)\n",
      "generating feature for wedding (train)\n",
      "generating feature for police (train)\n",
      "generating feature for murder (train)\n",
      "generating feature for martial arts (train)\n",
      "generating feature for biography (train)\n",
      "generating feature for dystopia (train)\n",
      "generating feature for comedy (val)\n",
      "generating feature for musical (val)\n",
      "generating feature for romance (val)\n",
      "generating feature for alien (test)\n",
      "generating feature for prison (test)\n",
      "generating feature for superhero (test)\n",
      "generating feature for extramarital (test)\n",
      "generating feature for magic (test)\n",
      "number of queries, train 12, test 5, validation 3\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(QUERY_LIST)\n",
    "train_queries = QUERY_LIST[0:12]\n",
    "val_queries = QUERY_LIST[12:15]\n",
    "test_queries = QUERY_LIST[15:]\n",
    "feat_suffixes = [\"train\", \"val\", \"test\"]\n",
    "test_qid2query = {}\n",
    "qid = 1\n",
    "for qt_idx, queries in enumerate([train_queries, val_queries, test_queries]):\n",
    "    fletor = open(FEATURE_FILE_TEMPLATE.format(feat_suffixes[qt_idx]), \"w\")\n",
    "    for query in queries:\n",
    "        print(\"generating feature for {:s} ({:s})\".format(query, feat_suffixes[qt_idx]))\n",
    "        if feat_suffixes[qt_idx] == \"test\":\n",
    "            test_qid2query[qid] = query\n",
    "        docs = get_search_results(query, 100)\n",
    "        # features from search result\n",
    "        orig_scores = [doc[\"score\"] for doc in docs]\n",
    "        title_sims_tfidf = get_tfidf_similarities(query, docs, \"title_t\")\n",
    "        desc_sims_tfidf = get_tfidf_similarities(query, docs, \"description_t\")\n",
    "        title_sims_bm25 = get_bm25_similarities(query, docs, \"title_t\")\n",
    "        desc_sims_bm25 = get_bm25_similarities(query, docs, \"description_t\")\n",
    "        doc_recencies = get_doc_recencies(docs, \"released_dt\")\n",
    "        doc_categories = get_doc_categories(docs)\n",
    "        for i in range(len(docs)):\n",
    "            doc = docs[i]\n",
    "            # get additional fields\n",
    "            doc_id = int(doc[\"id\"])\n",
    "            rating = doc[\"rating_f\"]\n",
    "            # write record\n",
    "            fletor.write(\"{:s}\\n\".format(format_letor(doc_id, rating, qid, query, orig_scores[i],\n",
    "                                                      title_sims_tfidf[i], desc_sims_tfidf[i],\n",
    "                                                      title_sims_bm25[i], desc_sims_bm25[i],\n",
    "                                                      doc_recencies[i], doc_categories[i])))\n",
    "        qid += 1\n",
    "    fletor.close()\n",
    "print(\"number of queries, train {:d}, test {:d}, validation {:d}\".format(\n",
    "    len(train_queries), len(test_queries), len(val_queries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Command to train LambdaMART model using the LETOR files generated in previous step is as follows:\n",
    "\n",
    "    java -jar RankLib-2.10.jar \\\n",
    "        -train ../data/diy_features_train.txt \\\n",
    "        -test ../data/diy_features_test.txt \\\n",
    "        -validate ../data/diy_features_val.txt \\\n",
    "        -ranker 6 \\\n",
    "        -metric2t NDCG@10 \\\n",
    "        -metric2T NDCG@10 \\\n",
    "        -norm zscore \\\n",
    "        -save ../models/diy_lambdamart_model.txt\n",
    "\n",
    "And the console output for this command is as follows:\n",
    "\n",
    "    Discard orig. features\n",
    "    Training data:\t../data/diy_features_train.txt\n",
    "    Test data:\t../data/diy_features_test.txt\n",
    "    Validation data:\t../data/diy_features_val.txt\n",
    "    Feature vector representation: Dense.\n",
    "    Ranking method:\tLambdaMART\n",
    "    Feature description file:\tUnspecified. All features will be used.\n",
    "    Train metric:\tNDCG@10\n",
    "    Test metric:\tNDCG@10\n",
    "    Feature normalization: zscore\n",
    "    Model file: ../models/diy_lambdamart_model.txt\n",
    "    \n",
    "    [+] LambdaMART's Parameters:\n",
    "    No. of trees: 1000\n",
    "    No. of leaves: 10\n",
    "    No. of threshold candidates: 256\n",
    "    Min leaf support: 1\n",
    "    Learning rate: 0.1\n",
    "    Stop early: 100 rounds without performance gain on validation data\n",
    "    \n",
    "    Reading feature file [../data/diy_features_train.txt]... [Done.]            \n",
    "    (12 ranked lists, 997 entries read)\n",
    "    Reading feature file [../data/diy_features_val.txt]... [Done.]            \n",
    "    (3 ranked lists, 295 entries read)\n",
    "    Reading feature file [../data/diy_features_test.txt]... [Done.]            \n",
    "    (5 ranked lists, 410 entries read)\n",
    "    Initializing... [Done]\n",
    "    ---------------------------------\n",
    "    Training starts...\n",
    "    ---------------------------------\n",
    "    #iter   | NDCG@10-T | NDCG@10-V | \n",
    "    ---------------------------------\n",
    "    1       | 0.5433    | 0.5499    | \n",
    "    2       | 0.6531    | 0.4939    | \n",
    "    3       | 0.6574    | 0.4581    | \n",
    "    4       | 0.6798    | 0.4575    | \n",
    "    5       | 0.6711    | 0.4583    | \n",
    "    6       | 0.6874    | 0.4859    | \n",
    "    7       | 0.6925    | 0.4855    | \n",
    "    8       | 0.6906    | 0.5006    | \n",
    "    9       | 0.698     | 0.5006    | \n",
    "    10      | 0.6852    | 0.5006    | \n",
    "    11      | 0.6864    | 0.5006    | \n",
    "    12      | 0.6908    | 0.5006    | \n",
    "    13      | 0.7036    | 0.5006    | \n",
    "    14      | 0.6948    | 0.5006    | \n",
    "    15      | 0.7087    | 0.5064    | \n",
    "    16      | 0.7099    | 0.4914    | \n",
    "    17      | 0.716     | 0.4914    | \n",
    "    18      | 0.708     | 0.4914    | \n",
    "    19      | 0.7167    | 0.4903    | \n",
    "    20      | 0.7271    | 0.4903    | \n",
    "    21      | 0.7255    | 0.4885    | \n",
    "    22      | 0.7305    | 0.4793    | \n",
    "    23      | 0.733     | 0.4793    | \n",
    "    24      | 0.7314    | 0.4793    | \n",
    "    25      | 0.7366    | 0.4821    | \n",
    "    26      | 0.7363    | 0.4884    | \n",
    "    27      | 0.7398    | 0.4766    | \n",
    "    28      | 0.741     | 0.4926    | \n",
    "    29      | 0.7389    | 0.492     | \n",
    "    30      | 0.7357    | 0.4861    | \n",
    "    31      | 0.7424    | 0.4789    | \n",
    "    32      | 0.7455    | 0.4802    | \n",
    "    33      | 0.7456    | 0.4904    | \n",
    "    34      | 0.7505    | 0.4876    | \n",
    "    35      | 0.7496    | 0.4951    | \n",
    "    36      | 0.7588    | 0.4954    | \n",
    "    37      | 0.7686    | 0.4964    | \n",
    "    38      | 0.768     | 0.4988    | \n",
    "    39      | 0.7698    | 0.4988    | \n",
    "    40      | 0.7725    | 0.5068    | \n",
    "    41      | 0.7754    | 0.5068    | \n",
    "    42      | 0.7778    | 0.5158    | \n",
    "    43      | 0.778     | 0.5177    | \n",
    "    44      | 0.7848    | 0.5189    | \n",
    "    45      | 0.7863    | 0.5278    | \n",
    "    46      | 0.7858    | 0.5269    | \n",
    "    47      | 0.7821    | 0.5298    | \n",
    "    48      | 0.7872    | 0.5408    | \n",
    "    49      | 0.7879    | 0.5316    | \n",
    "    50      | 0.7884    | 0.5641    | \n",
    "    51      | 0.7902    | 0.5627    | \n",
    "    52      | 0.7951    | 0.5639    | \n",
    "    53      | 0.7967    | 0.5639    | \n",
    "    54      | 0.7934    | 0.5606    | \n",
    "    55      | 0.7928    | 0.5644    | \n",
    "    56      | 0.7997    | 0.5734    | \n",
    "    57      | 0.7992    | 0.5684    | \n",
    "    58      | 0.8001    | 0.547     | \n",
    "    59      | 0.8034    | 0.5525    | \n",
    "    60      | 0.8065    | 0.5514    | \n",
    "    61      | 0.8064    | 0.5514    | \n",
    "    62      | 0.8062    | 0.5497    | \n",
    "    63      | 0.8122    | 0.529     | \n",
    "    64      | 0.8123    | 0.5381    | \n",
    "    65      | 0.8134    | 0.5529    | \n",
    "    66      | 0.8198    | 0.5532    | \n",
    "    67      | 0.82      | 0.5673    | \n",
    "    68      | 0.8206    | 0.5652    | \n",
    "    69      | 0.8219    | 0.566     | \n",
    "    70      | 0.8228    | 0.5682    | \n",
    "    71      | 0.8226    | 0.5675    | \n",
    "    72      | 0.8243    | 0.5695    | \n",
    "    73      | 0.8243    | 0.5755    | \n",
    "    74      | 0.8301    | 0.5783    | \n",
    "    75      | 0.827     | 0.5728    | \n",
    "    76      | 0.828     | 0.5728    | \n",
    "    77      | 0.8303    | 0.5787    | \n",
    "    78      | 0.8339    | 0.5799    | \n",
    "    79      | 0.8326    | 0.5774    | \n",
    "    80      | 0.8339    | 0.577     | \n",
    "    81      | 0.8342    | 0.5797    | \n",
    "    82      | 0.8291    | 0.5826    | \n",
    "    83      | 0.8379    | 0.5794    | \n",
    "    84      | 0.8382    | 0.5627    | \n",
    "    85      | 0.8417    | 0.5613    | \n",
    "    86      | 0.8447    | 0.5615    | \n",
    "    87      | 0.8468    | 0.5742    | \n",
    "    88      | 0.8449    | 0.5853    | \n",
    "    89      | 0.8429    | 0.5828    | \n",
    "    90      | 0.8477    | 0.5825    | \n",
    "    91      | 0.8497    | 0.586     | \n",
    "    92      | 0.8502    | 0.588     | \n",
    "    93      | 0.8495    | 0.5844    | \n",
    "    94      | 0.8537    | 0.5914    | \n",
    "    95      | 0.8532    | 0.5906    | \n",
    "    96      | 0.8522    | 0.5912    | \n",
    "    97      | 0.8531    | 0.5912    | \n",
    "    98      | 0.85      | 0.5912    | \n",
    "    99      | 0.8531    | 0.5912    | \n",
    "    100     | 0.8582    | 0.5912    | \n",
    "    101     | 0.8608    | 0.5697    | \n",
    "    102     | 0.8586    | 0.5806    | \n",
    "    103     | 0.8683    | 0.5849    | \n",
    "    104     | 0.8703    | 0.5802    | \n",
    "    105     | 0.8674    | 0.5907    | \n",
    "    106     | 0.8823    | 0.5743    | \n",
    "    107     | 0.8868    | 0.572     | \n",
    "    108     | 0.8855    | 0.5741    | \n",
    "    109     | 0.8907    | 0.5725    | \n",
    "    110     | 0.8923    | 0.5729    | \n",
    "    111     | 0.894     | 0.5717    | \n",
    "    112     | 0.8937    | 0.5725    | \n",
    "    113     | 0.8978    | 0.5775    | \n",
    "    114     | 0.8998    | 0.5784    | \n",
    "    115     | 0.8968    | 0.5787    | \n",
    "    116     | 0.9013    | 0.5787    | \n",
    "    117     | 0.9013    | 0.5754    | \n",
    "    118     | 0.8996    | 0.5745    | \n",
    "    119     | 0.9004    | 0.5659    | \n",
    "    120     | 0.9015    | 0.5659    | \n",
    "    121     | 0.9079    | 0.5729    | \n",
    "    122     | 0.9094    | 0.5537    | \n",
    "    123     | 0.9152    | 0.5525    | \n",
    "    124     | 0.9127    | 0.5576    | \n",
    "    125     | 0.9128    | 0.5588    | \n",
    "    126     | 0.9124    | 0.5584    | \n",
    "    127     | 0.9177    | 0.5584    | \n",
    "    128     | 0.9135    | 0.5584    | \n",
    "    129     | 0.9197    | 0.5584    | \n",
    "    130     | 0.9181    | 0.5584    | \n",
    "    131     | 0.9199    | 0.5574    | \n",
    "    132     | 0.9248    | 0.558     | \n",
    "    133     | 0.9235    | 0.5568    | \n",
    "    134     | 0.9272    | 0.5568    | \n",
    "    135     | 0.9263    | 0.5558    | \n",
    "    136     | 0.9266    | 0.5599    | \n",
    "    137     | 0.9289    | 0.5558    | \n",
    "    138     | 0.9304    | 0.5559    | \n",
    "    139     | 0.9306    | 0.5561    | \n",
    "    140     | 0.9308    | 0.5558    | \n",
    "    141     | 0.9372    | 0.5544    | \n",
    "    142     | 0.9373    | 0.5544    | \n",
    "    143     | 0.9349    | 0.5498    | \n",
    "    144     | 0.9413    | 0.5498    | \n",
    "    145     | 0.9448    | 0.5498    | \n",
    "    146     | 0.945     | 0.5602    | \n",
    "    147     | 0.9451    | 0.5588    | \n",
    "    148     | 0.9453    | 0.5585    | \n",
    "    149     | 0.9455    | 0.5585    | \n",
    "    150     | 0.9444    | 0.5608    | \n",
    "    151     | 0.9495    | 0.565     | \n",
    "    152     | 0.957     | 0.5695    | \n",
    "    153     | 0.9605    | 0.5691    | \n",
    "    154     | 0.9608    | 0.5798    | \n",
    "    155     | 0.9647    | 0.5711    | \n",
    "    156     | 0.9684    | 0.5715    | \n",
    "    157     | 0.9691    | 0.5715    | \n",
    "    158     | 0.9716    | 0.5702    | \n",
    "    159     | 0.9698    | 0.5702    | \n",
    "    160     | 0.9705    | 0.5702    | \n",
    "    161     | 0.9716    | 0.5543    | \n",
    "    162     | 0.9715    | 0.5543    | \n",
    "    163     | 0.9742    | 0.5477    | \n",
    "    164     | 0.9742    | 0.5553    | \n",
    "    165     | 0.976     | 0.5553    | \n",
    "    166     | 0.976     | 0.5386    | \n",
    "    167     | 0.9768    | 0.5386    | \n",
    "    168     | 0.9767    | 0.5478    | \n",
    "    169     | 0.9765    | 0.5386    | \n",
    "    170     | 0.9767    | 0.5482    | \n",
    "    171     | 0.9768    | 0.5478    | \n",
    "    172     | 0.9749    | 0.5478    | \n",
    "    173     | 0.9773    | 0.5402    | \n",
    "    174     | 0.9773    | 0.5406    | \n",
    "    175     | 0.9771    | 0.5406    | \n",
    "    176     | 0.9776    | 0.5482    | \n",
    "    177     | 0.9776    | 0.5482    | \n",
    "    178     | 0.9765    | 0.5482    | \n",
    "    179     | 0.9791    | 0.5483    | \n",
    "    180     | 0.9795    | 0.5492    | \n",
    "    181     | 0.9798    | 0.5492    | \n",
    "    182     | 0.9798    | 0.5486    | \n",
    "    183     | 0.9798    | 0.5486    | \n",
    "    184     | 0.9798    | 0.5527    | \n",
    "    185     | 0.9798    | 0.5518    | \n",
    "    186     | 0.9798    | 0.5535    | \n",
    "    187     | 0.9798    | 0.5564    | \n",
    "    188     | 0.9781    | 0.5509    | \n",
    "    189     | 0.9781    | 0.5368    | \n",
    "    190     | 0.9801    | 0.5326    | \n",
    "    191     | 0.9801    | 0.5418    | \n",
    "    192     | 0.9789    | 0.5429    | \n",
    "    193     | 0.9807    | 0.5429    | \n",
    "    194     | 0.9812    | 0.5418    | \n",
    "    195     | 0.9812    | 0.5339    | \n",
    "    ---------------------------------\n",
    "    Finished sucessfully.\n",
    "    NDCG@10 on training data: 0.8537\n",
    "    NDCG@10 on validation data: 0.5914\n",
    "    ---------------------------------\n",
    "    NDCG@10 on test data: 0.4409\n",
    "    \n",
    "    Model saved to: ../models/diy_lambdamart_model.txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Trained Model\n",
    "\n",
    "Since the index does not provide any LTR support, we don't have to upload the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run rerank Query\n",
    "\n",
    "On the other hand, we cannot use the index to run a rerank query, so we need to run inference on the trained model directly.\n",
    "\n",
    "In our case study, we have chosen to run it from the command line as shown below, but for a production system, this can be done by hooking into RankLib's inference mechanism, since it is written in Java and open source.\n",
    "\n",
    "    cd <scripts_dir>\n",
    "    java -jar RankLib-2.10.jar \\\n",
    "        -load ../data/diy_lambdamart_model.txt \\\n",
    "        -rank ../data/diy_features_test.txt \\\n",
    "        -norm zscore \\\n",
    "        -score ../data/diy_lambdamart_scores.txt\n",
    "\n",
    "which returns the following console output:\n",
    "\n",
    "    Discard orig. features\n",
    "    Model file:\t../models/diy_lambdamart_model.txt\n",
    "    Feature normalization: zscore\n",
    "    Model:\t\tLambdaMART\n",
    "    Reading feature file [../data/diy_features_test.txt]... [Done.]            \n",
    "    (5 ranked lists, 410 entries read)\n",
    "\n",
    "and writes out the score file as tab separated (qid, doc_id, score) triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★★☆☆\n",
      "★★★★☆\n"
     ]
    }
   ],
   "source": [
    "def rating2label(rating):\n",
    "    \"\"\" convert 0-10 continuous rating to 1-5 categorical labels \"\"\"\n",
    "    return int(rating // 2) + 1\n",
    "\n",
    "\n",
    "def get_rating_string(rating):\n",
    "    rating_string = []\n",
    "    for i in range(rating):\n",
    "        rating_string.append(u\"\\u2605\")\n",
    "    for i in range(5 - rating):\n",
    "        rating_string.append(u\"\\u2606\")\n",
    "    return \"\".join(rating_string)\n",
    "\n",
    "\n",
    "print(get_rating_string(3))\n",
    "print(get_rating_string(rating2label(6.4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_results(docs, query, top_n):\n",
    "    print(\"top {:d} results for {:s}\".format(top_n * 2, query))\n",
    "    print(\"---\")\n",
    "    for doc in docs[0:top_n * 2]:\n",
    "        doc_id = int(doc[\"id\"])\n",
    "        stars = get_rating_string(rating2label(float(doc[\"rating_f\"])))\n",
    "        score = float(doc[\"score\"])\n",
    "        title = doc[\"title_t\"]\n",
    "        print(\"{:s} {:06d} {:.3f} {:s}\".format(stars, doc_id, score, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "magic\n"
     ]
    }
   ],
   "source": [
    "test_qids = list(test_qid2query.keys())\n",
    "qid = random.randint(min(test_qids), max(test_qids))\n",
    "query = test_qid2query[qid]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 without LTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 results for magic\n",
      "---\n",
      "★★★☆☆ 139519 9.781 Magic Magic\n",
      "★★★★☆ 034193 8.863 Magic\n",
      "★★★★☆ 032916 7.557 Scooby-Doo! Abracadabra-Doo\n",
      "★★★★☆ 070313 7.519 Bowery to Bagdad\n",
      "★★★★☆ 181533 7.514 Night at the Museum: Secret of the Tomb\n",
      "★★★★☆ 045671 7.468 Rough Magic\n",
      "★★★★☆ 006435 7.468 Practical Magic\n",
      "★★★★☆ 068894 7.468 Shadow Magic\n",
      "★★★★☆ 037204 7.468 Summer Magic\n",
      "★★★★★ 038464 7.468 Black Magic\n",
      "★★☆☆☆ 024797 7.468 Magic Man\n",
      "★★★★☆ 057211 7.468 Magic Trip\n",
      "★★★☆☆ 215405 7.468 Magic Kid\n",
      "★★★★☆ 203179 7.468 Magic Camp\n",
      "★★★★☆ 077930 7.468 Magic Mike\n",
      "★★★★☆ 090966 7.468 Magic Town\n",
      "★★★★☆ 092796 7.468 Black Magic\n",
      "★★★★☆ 302429 7.468 Strange Magic\n",
      "★★☆☆☆ 029419 7.468 Carnival Magic\n",
      "★★★★☆ 081420 7.468 Christmas Magic\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"q\": query,\n",
    "    \"defType\": \"edismax\",\n",
    "    \"qf\": \"title_t description_t\",\n",
    "    \"pf\": \"title_t description_t\",\n",
    "    \"mm\": 2,\n",
    "    \"fl\": \"id,title_t,rating_f,score\",            \n",
    "    \"rows\": TOP_N * 10\n",
    "}\n",
    "params = urllib.parse.urlencode(payload, quote_via=urllib.parse.quote_plus)\n",
    "search_url = SOLR_URL + \"/select?\" + params\n",
    "resp = requests.get(search_url)\n",
    "resp_json = json.loads(resp.text)\n",
    "docs = resp_json[\"response\"][\"docs\"]\n",
    "render_results(docs, query, TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 reranked with LTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscores = open(SCORE_FILE, \"r\")\n",
    "rows = []\n",
    "doc_idx = 0\n",
    "for line in fscores:\n",
    "    line = line.strip()\n",
    "    rqid, doc_id, score = line.split(\"\\t\")\n",
    "    if int(rqid) != qid:\n",
    "        continue\n",
    "    rows.append((doc_idx, float(score)))\n",
    "    doc_idx += 1\n",
    "fscores.close()\n",
    "reranked_rows = sorted(rows, key=operator.itemgetter(1), reverse=True)[0:TOP_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 results for magic\n",
      "---\n",
      "★★★★☆ 388192 3.724 Siccîn 2\n",
      "★★★★☆ 038191 2.130 The Sunchaser\n",
      "★★★☆☆ 160220 1.637 Bronies: The Extremely Unexpected Adult Fans of My Little Pony\n",
      "★★★☆☆ 206145 1.540 Tarzan's Magic Fountain\n",
      "★☆☆☆☆ 243565 1.457 Mother Carey's Chickens\n",
      "★★★★☆ 000673 1.441 Harry Potter and the Prisoner of Azkaban\n",
      "★★★★☆ 181533 1.422 Night at the Museum: Secret of the Tomb\n",
      "★★★☆☆ 224141 1.365 Into the Woods\n",
      "★★☆☆☆ 181426 1.326 Chandu on the Magic Island\n",
      "★★★★☆ 015400 1.306 Mickey's Once Upon a Christmas\n",
      "★★★☆☆ 139519 9.781 Magic Magic\n",
      "★★★★☆ 034193 8.863 Magic\n",
      "★★★★☆ 032916 7.557 Scooby-Doo! Abracadabra-Doo\n",
      "★★★★☆ 070313 7.519 Bowery to Bagdad\n",
      "★★★★☆ 181533 1.422 Night at the Museum: Secret of the Tomb\n",
      "★★★★☆ 045671 7.468 Rough Magic\n",
      "★★★★☆ 006435 7.468 Practical Magic\n",
      "★★★★☆ 068894 7.468 Shadow Magic\n",
      "★★★★☆ 037204 7.468 Summer Magic\n",
      "★★★★★ 038464 7.468 Black Magic\n"
     ]
    }
   ],
   "source": [
    "reranked_docs = []\n",
    "# LTR layer\n",
    "for doc_id, score in reranked_rows:\n",
    "    doc = docs[doc_id]\n",
    "    doc[\"score\"] = score\n",
    "    reranked_docs.append(doc)\n",
    "# rest of the results\n",
    "doc_ids_to_remove = set([x[0] for x in reranked_rows])\n",
    "for doc in docs:\n",
    "    doc_id = int(doc[\"id\"])\n",
    "    if doc_id in doc_ids_to_remove:\n",
    "        continue\n",
    "    reranked_docs.append(doc)\n",
    "    \n",
    "render_results(reranked_docs, query, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
