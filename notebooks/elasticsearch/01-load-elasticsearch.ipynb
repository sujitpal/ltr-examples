{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Index into Elasticsearch\n",
    "\n",
    "We are using ES 6.3 + ES-LTR plugin. LTR plugin was installed using the following command:\n",
    "\n",
    "    cd <elasticsearch_home>\n",
    "    bin/elasticsearch-plugin install http://es-learn-to-rank.labs.o19s.com/ltr-1.1.0-es6.3.1.zip\n",
    "    \n",
    "We are using [elasticsearch-head](https://github.com/mobz/elasticsearch-head) as our browser client. In order to make it work, you need to disable some security measures using the following directives in `config/elasticsearch.yml`.\n",
    "\n",
    "    http.cors.enabled: true\n",
    "    http.cors.allow-origin: \"*\"\n",
    "\n",
    "First step is to set up an index `tmdbindex` and load the data into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../data/tmdb-dataset\"\n",
    "\n",
    "MOVIES_DATA = os.path.join(DATA_DIR, \"movies_metadata.csv\")\n",
    "LOOKUPS_DB = os.path.join(DATA_DIR, \"lookups.db\")\n",
    "\n",
    "ES_URL = \"http://localhost:9200\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"tmdbindex\"}\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"settings\" : {\n",
    "        \"index\" : {\n",
    "            \"number_of_shards\" : 1, \n",
    "            \"number_of_replicas\" : 1 \n",
    "        }\n",
    "    },\n",
    "    \"mappings\" : {\n",
    "        \"doc\" : {\n",
    "            \"properties\" : {\n",
    "                \"doc_id\": { \"type\": \"keyword\", \"store\": \"true\" },\n",
    "                \"title\": { \"type\": \"text\", \"store\": \"true\", \"copy_to\": \"title_tfidf\" },\n",
    "                \"description\": { \"type\": \"text\", \"store\": \"true\", \"copy_to\": \"description_tfidf\" },\n",
    "                \"title_tfidf\": { \"type\": \"text\", \"store\": \"true\", \"similarity\": \"classic\" },\n",
    "                \"description_tfidf\": { \"type\": \"text\", \"store\": \"true\", \"similarity\": \"classic\" },\n",
    "                \"popularity\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"release_dt\": { \"type\": \"date\", \"store\": \"true\" },\n",
    "                \"revenue\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"runtime\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"rating\": { \"type\": \"double\", \"store\": \"true\" },\n",
    "                \"keywords\": { \"type\": \"keyword\", \"store\": \"true\" },\n",
    "                \"genres\": { \"type\": \"keyword\", \"store\": \"true\" }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "resp = requests.put(ES_URL + \"/tmdbindex\", headers=headers, data=json.dumps(data))\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(conn, movie_id):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select keywords from keywords where mid = ?\", [movie_id])\n",
    "    rows = cur.fetchall()\n",
    "    keywords = []\n",
    "    if len(rows) > 0:\n",
    "        for row in rows:\n",
    "            keywords = row[0].split(\"|\")\n",
    "            break\n",
    "    cur.close()\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def filter_genres(conn, genres):\n",
    "    filtered_genres = []\n",
    "    cur = conn.cursor()\n",
    "    for genre in genres:\n",
    "        cur.execute(\"select gname from genres where gname = ?\", [genre])\n",
    "        rows = cur.fetchall()\n",
    "        if len(rows) == 0:\n",
    "            continue\n",
    "        filtered_genres.append(genre)\n",
    "    cur.close()\n",
    "    return filtered_genres\n",
    "\n",
    "\n",
    "def get_float(orig_value, default_value):\n",
    "    if orig_value is None:\n",
    "        return default_value\n",
    "    elif len(orig_value.strip()) == 0:\n",
    "        return default_value\n",
    "    else:\n",
    "        return float(orig_value)\n",
    "\n",
    "\n",
    "def parse_genres(genre_json):\n",
    "    if len(genre_json.strip()) == 0:\n",
    "        return []\n",
    "    names = []\n",
    "    idname_pairs = json.loads(genre_json.replace(\"'\", \"\\\"\"))\n",
    "    for idname_pair in idname_pairs:\n",
    "        names.append(idname_pair[\"name\"])\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_record_to_es(es_url, doc_id, title, description, popularity, \n",
    "                     release_date, revenue, runtime, rating, keywords, genres,\n",
    "                     should_commit=False):\n",
    "    if doc_id is not None:\n",
    "        doc = {\n",
    "            \"doc_id\": str(doc_id),\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"popularity\": popularity,\n",
    "            \"release_dt\": release_date,\n",
    "            \"revenue\": revenue,\n",
    "            \"runtime\": runtime,\n",
    "            \"rating\": rating,\n",
    "            \"keywords\": keywords,\n",
    "            \"genres\": genres\n",
    "        }\n",
    "        resp = requests.put(es_url + \"/tmdbindex/doc/{:d}\".format(doc_id), \n",
    "                            headers=headers, data=json.dumps(doc))\n",
    "    if should_commit:\n",
    "        requests.post(es_url + \"/tmdbindex/_flush\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records ingested into Elasticsearch\n",
      "1000 records ingested into Elasticsearch\n",
      "2000 records ingested into Elasticsearch\n",
      "3000 records ingested into Elasticsearch\n",
      "4000 records ingested into Elasticsearch\n",
      "5000 records ingested into Elasticsearch\n",
      "6000 records ingested into Elasticsearch\n",
      "7000 records ingested into Elasticsearch\n",
      "8000 records ingested into Elasticsearch\n",
      "9000 records ingested into Elasticsearch\n",
      "10000 records ingested into Elasticsearch\n",
      "11000 records ingested into Elasticsearch\n",
      "12000 records ingested into Elasticsearch\n",
      "13000 records ingested into Elasticsearch\n",
      "14000 records ingested into Elasticsearch\n",
      "15000 records ingested into Elasticsearch\n",
      "16000 records ingested into Elasticsearch\n",
      "17000 records ingested into Elasticsearch\n",
      "18000 records ingested into Elasticsearch\n",
      "19000 records ingested into Elasticsearch\n",
      "20000 records ingested into Elasticsearch\n",
      "21000 records ingested into Elasticsearch\n",
      "22000 records ingested into Elasticsearch\n",
      "23000 records ingested into Elasticsearch\n",
      "24000 records ingested into Elasticsearch\n",
      "25000 records ingested into Elasticsearch\n",
      "26000 records ingested into Elasticsearch\n",
      "27000 records ingested into Elasticsearch\n",
      "28000 records ingested into Elasticsearch\n",
      "29000 records ingested into Elasticsearch\n",
      "30000 records ingested into Elasticsearch\n",
      "31000 records ingested into Elasticsearch\n",
      "32000 records ingested into Elasticsearch\n",
      "33000 records ingested into Elasticsearch\n",
      "34000 records ingested into Elasticsearch\n",
      "35000 records ingested into Elasticsearch\n",
      "36000 records ingested into Elasticsearch\n",
      "37000 records ingested into Elasticsearch\n",
      "38000 records ingested into Elasticsearch\n",
      "39000 records ingested into Elasticsearch\n",
      "40000 records ingested into Elasticsearch\n",
      "41000 records ingested into Elasticsearch\n",
      "42000 records ingested into Elasticsearch\n",
      "43000 records ingested into Elasticsearch\n",
      "44000 records ingested into Elasticsearch\n",
      "45000 records ingested into Elasticsearch\n",
      "45466 records ingested into Elasticsearch, COMPLETE\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(LOOKUPS_DB)\n",
    "i = 0\n",
    "should_commit = False\n",
    "with open(MOVIES_DATA, \"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"{:d} records ingested into Elasticsearch\".format(i))\n",
    "            should_commit = True\n",
    "        if row[\"original_language\"] != \"en\":\n",
    "            # only stick to english\n",
    "            i += 1\n",
    "            continue\n",
    "        doc_id = int(row[\"id\"])\n",
    "        title = row[\"original_title\"]\n",
    "        description = row[\"overview\"]\n",
    "        popularity = get_float(row[\"popularity\"], 0.0)\n",
    "        release_date = row[\"release_date\"]\n",
    "        revenue = get_float(row[\"revenue\"], 0.0)\n",
    "        runtime = get_float(row[\"runtime\"], 0.0)\n",
    "        rating = get_float(row[\"vote_average\"], 0.0)\n",
    "        # look up keywords\n",
    "        keywords = get_keywords(conn, doc_id)\n",
    "        # parse out genres\n",
    "        genres = filter_genres(conn, parse_genres(row[\"genres\"]))\n",
    "        # add record to solr\n",
    "        add_record_to_es(ES_URL, doc_id, title, description, popularity, \n",
    "                         release_date, revenue, runtime, rating, keywords, genres,\n",
    "                         should_commit=should_commit)\n",
    "        should_commit = False\n",
    "        i += 1\n",
    "\n",
    "add_record_to_es(ES_URL, None, None, None, None, None, None, None, None, None, None, True)\n",
    "print(\"{:d} records ingested into Elasticsearch, COMPLETE\".format(i))\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
